[["index.html", "Quantile Regression: Facts and digressions 1 Exordium", " Quantile Regression: Facts and digressions Eduardo Horta Department of Statistics — UFRGSeduardo.horta@ufrgs.br 2021-06-27 1 Exordium These notes are intended as a place where I’ll gather interesting facts about quantiles and quantile regression (from elementary to fringe). This is definitely not a textbook: for that you can consult Koenker (2005) and Koenker et al. (2017). In any case I have included many exercises that clarify curious and relevant facts that appear scattered in the literature. This notes are a work in progress, and likely will always be: whenever I deem a topic worthy of appearing here, I’ll add it sooner or later. The github repository for the notes can be found at https://github.com/eduardohorta/QuantRegFacts. It is assumed that the reader has some knowledge of Probability and Statistics. The expression “measurable function” appears many times in the text; if you don’t know what it means, substitute “measurable” by “piecewise continuous.” You won’t lose much. In the Probability and Statistics literature, it has become a habit to adopt the notational abuse of writing \\(g(X)\\) when one really means the composition \\(g \\circ X.\\) There is no easy way around,1 and sometimes ambiguity does arise.2 At any rate, in most cases one can tell from context what is the correct interpretation, that is, in writing \\(g(X)\\) we usually know beforehand what the symbol \\(g\\) stands for. For example, if \\(X\\) is a random variable and \\(g\\) is a real valued measurable function on \\(\\mathbb{R},\\) then \\(g(X)\\) stands for \\(g\\circ X.\\) Similarly, if \\(g\\) is a functional or operator, say \\(g = \\mathbf{E},\\) then \\(\\mathbf{E}(X)\\) is not a composition but evaluation of the functional \\(\\mathbf{E}\\) at the “point” \\(X.\\) In view of this, I shall (in dismay) go along with tradition. We could give up tradition and go on with the more correct \\(g\\circ X\\) but the notation can get clumsy, especially if \\(g\\) is a function of many variables. Moreover, tradition exists — at least in part — for good reasons we do not fully understand.↩︎ For instance, for a random vector \\(X\\) the notation \\(\\Vert X\\Vert_2\\) can be used both for the “random Euclidean norm” \\(\\Vert X\\Vert_2 = \\sqrt{\\sum\\nolimits_{d=1}^{\\mathrm{D}_X}X_d^2}\\) and for the proper \\(L^2\\) norm, \\(\\Vert X\\Vert_2 = \\mathbf{E}\\sqrt{\\sum\\nolimits_{d=1}^{\\mathrm{D}_X}X_d^2}.\\)↩︎ "],["intro.html", "2 Intro: distributions and quantiles", " 2 Intro: distributions and quantiles In Science, probability distributions are one of the main ways through which uncertainty about phenomena is modeled and quantified. In the most basic setting, we have a random variable, say \\(Y,\\) that represents the numerical outcome of an experiment. Formally, \\(Y\\) is modeled as a measurable function defined on some (abstract/mathematical) measurable space \\((\\Omega,\\mathscr{F}).\\) Each probability measure \\(\\mathbf{P}\\) on \\((\\Omega,\\mathscr{F})\\) then induces a probability measure on \\(\\mathbb{R},\\) called the distribution of \\(Y\\) and denoted by \\(\\mathbf{P}_Y,\\) defined through \\[\\begin{equation} \\mathbf{P}_Y(B) := \\mathbf{P}[Y\\in B] \\tag{2.1} \\end{equation}\\] for each Borel subset \\(B\\subseteq\\mathbb{R}.\\) Importantly, as a consequence of Carathéodory’s Extension Theorem, the measure \\(\\mathbf{P}_Y\\) is recoverable from a much simpler function, namely the cumulative distribution function of \\(Y,\\) denoted by \\(F_Y\\) (of course, \\(F_Y\\) depends implicitly on \\(\\mathbf{P}\\)) and defined by \\[\\begin{equation} F_Y(y) := \\mathbf{P}_Y(-\\infty,y] = \\mathbf{P}[Y\\le y],\\qquad y\\in\\mathbb{R}. \\end{equation}\\] The preceding assertion means that the task of cooking up a probability distribution for a scalar random variable \\(Y\\) boils down to exhibiting its cumulative distribution function. This is nice because probability measures are not computationally tractable, whereas cumulative distribution functions, being representable through algebraic expressions or at least via numerical formulas, are. To sum up, the message is that if we want to build a model to quantify uncertainty about a scalar phenomenon, all we have to do is to propose a non-decrasing, right-continuous function \\(F\\colon\\mathbb{R}\\to\\mathbb{R}\\) that satisfies the requirements \\(\\lim_{y\\to -\\infty}F(y) = 0\\) and \\(\\lim_{y\\to+\\infty}F(y) = 1.\\) Such functions exhaust, in a one-to-one fashion, the category of univariate probability measures. Example 2.1 Let \\(f\\colon \\mathbb{R}\\to \\mathbb{R}\\) be defined through \\[\\begin{equation} f(y) := \\frac{1}{\\sqrt{2\\pi}}\\mathrm{e}^{-y^2/2},\\qquad y\\in\\mathbb{R}. \\end{equation}\\] Put \\(\\Omega = \\mathbb{R}\\) and let \\(\\mathscr{F}\\) be the class of Borel subsets of \\(\\Omega.\\) Also, define \\[\\begin{equation} F(y) := \\int_{-\\infty}^{y} f(u)\\,\\mathrm{d}u,\\qquad y\\in \\mathbb{R}. \\end{equation}\\] If we now let \\(\\mathbf{P}\\) be the unique probability measure (given by Carathéodory’s Theorem) on \\((\\Omega,\\mathscr{F})\\) satisfying the equality \\(\\mathbf{P}(-\\infty,y] = F(y)\\) for all \\(y\\in\\mathbb{R}\\) , then \\(\\mathbf{P}\\) is called the standard Gaussian distribution on \\(\\mathbb{R}\\). Additionally, defining the random variable \\(Y\\) through \\[\\begin{equation} Y(\\omega) := \\omega,\\qquad \\omega\\in\\mathbb{R}, \\end{equation}\\] it is clear that \\(Y\\) has distribution function \\(\\mathbf{P}\\) and cumulative distribution function \\(F,\\) that is, \\(\\mathbf{P}_Y = \\mathbf{P}\\) and \\(F_Y = F.\\) Such \\(Y\\) is called a standard Gaussian (or: standard Normal) random variable. "],["quantiles-and-ranks.html", "2.1 Quantiles and ranks", " 2.1 Quantiles and ranks We now show that there is an alternative approach to specify a probability measure on \\(\\mathbb{R}.\\) Let \\(Y\\) be a random variable with cumulative distribution function \\(F_Y.\\) For \\(\\tau\\in(0,1),\\) the \\(\\tau\\)-th quantile of \\(Y\\) is the real number \\(Q_Y(\\tau)\\) defined via \\[\\begin{equation} Q_Y(\\tau) := \\inf\\{y\\in\\mathbb{R}\\colon\\, F_Y(y)\\ge\\tau\\}. \\end{equation}\\] The function \\(\\tau \\mapsto Q_Y(\\tau)\\) from \\((0,1)\\) to \\(\\mathbb{R}\\) is called the quantile function of \\(Y\\) (also known as its generalized inverse of \\(F_Y\\)). Writing \\(F = F_Y\\) and \\(Q = Q_Y\\) for simplicity, it is an exercise (see van der Vaart (1998), Chapter 21) to check that \\(Q\\) is non-decreasing and left-continuous and that, for any \\(y\\in\\mathbb{R}\\) and \\(\\tau\\in(0,1),\\) one has \\[\\begin{equation} Q(\\tau)\\le y \\quad \\textsf{if and only if}\\quad \\tau\\le F(y). \\end{equation}\\] The latter equivalence has at least two important consequences: first, it shows that3 \\(F(y) = \\sup\\{\\tau\\in(0,1)\\colon\\,Q(\\tau)\\le y\\}\\) and so \\(F\\) and \\(Q\\) are entirely recoverable from one another: knowing \\(F\\) we know \\(Q\\) and vice-versa. Second, one sees that whenever \\(U\\) is a uniform random variable on the unit interval \\([0,1),\\) it holds that \\(Q(U)\\) has cumulative distribution function \\(F,\\) that is, \\(Q(U)\\) and \\(Y\\) are equal in distribution. This result is sometimes called the Fundamental Theorem of Simulation.4 In the same spirit as in the preceding section, we can say that if our aim is to build a model to quantify uncertainty about a scalar phenomenon, all we have to do is to propose a non-decrasing, left-continuous function \\(Q\\colon(0,1)\\to\\mathbb{R}.\\) Such functions exhaust, in a one-to-one fashion, the category of univariate probability measures. We have seen that univariate probability distributions are entirely characterized by their cumulative distribution functions, which in turn are entirely characterized by their corresponding quantile functions. A quantile function, notwithstanding, has the additional benefit of being not only a univariate probability model, but also a recipe to simulate from that model. Example 2.2 Fix a real number \\(\\lambda&gt;0,\\) and let \\(Q\\colon(0,1)\\to\\mathbb{R}\\) be defined by \\[\\begin{equation} Q(\\tau) := -\\frac{\\log(1-\\tau)}\\lambda,\\qquad \\tau\\in(0,1). \\end{equation}\\] Notice that \\(Q\\) is a quantile function, being strictly increasing and continuous. Now let \\(\\Omega := [0,1),\\) put \\(\\mathscr{F} := \\hphantom{\\!}\\) “the collection of Borel subsets of \\(\\Omega\\),” and let \\(\\mathbf{P}\\) be the Lebesge measure on \\((\\Omega,\\mathscr{F}),\\) that is, \\(\\mathbf{P}\\) is the unique Borel probability measure on \\([0,1)\\) for which the identity \\(\\mathbf{P}[a,b)=b-a\\) holds for all \\(0\\le a&lt;b\\le1.\\) In this setup, define the random variable \\(Y\\) through \\[\\begin{equation} Y(\\omega) := Q(\\omega),\\qquad \\omega\\in\\Omega. \\end{equation}\\] What is the distribution of \\(Y\\)? We can compute this directly: for \\(y\\in\\mathbb{R},\\) we have \\[\\begin{align} \\mathbf{P}[Y &gt; y] &amp;= \\mathbf{P}\\{\\omega\\in[0,1)\\colon\\, -\\log(1-\\omega)&gt;\\lambda y\\}\\\\ &amp;= \\mathbf{P}\\{\\omega\\in[0,1)\\colon\\, \\omega &gt; 1 - \\mathrm{e}^{-\\lambda y}\\}. \\end{align}\\] Therefore, \\(\\mathbf{P}[Y&gt;y] = 1\\) for \\(y\\le0\\) and, for \\(y&gt;0,\\) \\[\\begin{align} \\mathbf{P}[Y&gt;y] = \\mathbf{P}[1-\\mathrm{e}^{-\\lambda y}, 1) = 1 - (1-\\mathrm{e}^{-\\lambda y}) \\end{align}\\] by the definition of the Lebesgue measure \\(\\mathbf{P}.\\) It follows that \\[ F_Y(y) = 1-\\mathrm{e}^{-\\lambda y},\\qquad y\\ge0 \\] and \\(F_Y(y) = 0\\) otherwise: \\(Y\\) is an Exponential random variable with parameter \\(\\lambda\\). Of course, a straightforward computation will tell us that \\(Q_Y = Q.\\) Accordingly, if we define the random variable \\(U\\) to be the identity function on \\(\\Omega,\\) then clearly \\(U\\) has a Uniform\\([0,1]\\) distribution, and in this particular example not only are \\(Q(U)\\) and \\(Y\\) equal in distribution as we stressed earlier, but actually \\(Q(U) = Y\\) pointwise. In any case, the fact that \\(Y = Q(U)\\) allows us to simulate “from the distribution \\(F_Y\\),” as long as we have available a mechanism to generate pseudo-random uniform random variables. The r code below illustrates the idea, sampling \\(n\\) independent Uniform\\([0,1]\\) pseudo-random numbers \\(u_1,\\dots,u_n\\) and displaying the histogram plot of \\(Q(u_1),\\dots,Q(u_n).\\) n = 10000 lambda = 1 Usample = runif(n) Q = function(w) -log(1 - w)/lambda hist(Q(Usample), probability = TRUE, border = NA, breaks = &quot;Scott&quot;, xlab = &quot;y&quot;, main = NA) Exercise 2.1 Show that \\(\\mathbf{E}(Y) = \\int_0^1 Q_Y(\\tau)\\,\\mathrm{d}\\tau.\\)5 Exercise 2.2 Show that: \\(\\mathbf{P}[Y\\le Q_Y(\\tau)]\\ge \\tau\\) and \\(\\mathbf{P}[Y\\ge Q_Y(\\tau)]\\ge 1-\\tau.\\) \\(F_Y(Q_Y(\\tau)) = \\tau\\) if and only if \\(F_Y\\) is continuous at \\(Q_Y(\\tau).\\) Exercise 2.3 For \\(\\tau\\in(0,1),\\) let \\(\\rho_\\tau\\colon\\mathbb{R}\\to\\mathbb{R}\\) be defined through \\[\\begin{equation} \\rho_\\tau(u) = u(\\tau - \\mathbb{I}[u&lt;0]),\\quad u\\in\\mathbb{R}. \\end{equation}\\] Show that \\(2\\rho_\\tau(u) = (2\\tau-1)u + |u|.\\) Show that \\(\\rho_\\tau(u)\\) is a convex function of \\(u.\\) Show that \\(Y\\) is integrable if and only if \\(\\rho_\\tau(Y-y)\\) is integrable for all \\(y\\in\\mathbb{R}.\\) Show that \\(\\rho_\\tau(Y - y) - \\rho_\\tau(Y)\\) is integrable for all \\(y\\in\\mathbb{R}.\\)6 Show that, for all \\(y\\in\\mathbb{R},\\) one has \\[\\begin{equation} \\mathbf{E}\\rho_\\tau\\big(Y - Q_Y(\\tau)\\big) \\le \\mathbf{E}\\rho_\\tau(Y - y). \\end{equation}\\] Thus, the problem of minimizing the function \\(y\\mapsto \\mathbf{E}\\rho_\\tau(Y -y)\\) has at least one solution, and \\(Q_Y(\\tau)\\in\\arg\\min_y \\mathbf{E}\\rho_\\tau(Y-y).\\) Hint: consider first the case when \\(Y\\) is absolutely continuous with density function \\(f_Y.\\) The general case follows from the fact that \\(y\\mapsto \\mathbf{E}\\rho_\\tau(Y-y)\\) is a convex function (verify this assertion!), and that if \\(g\\colon\\mathbb{R}\\to\\mathbb{R}\\) is a convex function then \\(g(y^*)\\) is a global minimum of \\(g\\) if and only if \\(0\\) is an element of the subdifferential of \\(g\\) at \\(y^*\\) — see Lemma 7.10 in Aliprantis and Border (2006). Show that, if \\(g\\colon(0,1)\\to\\mathbb{R}\\) is any measurable function, then \\[\\begin{equation} \\int_0^1\\mathbf{E}\\rho_\\tau\\big(Y - Q_Y(\\tau)\\big)\\,\\mathrm{d}\\tau \\le \\int_0^1\\mathbf{E}\\rho_\\tau\\big(Y - g(\\tau)\\big)\\,\\mathrm{d}\\tau. \\end{equation}\\] Exercise 2.4 Assume \\(Y\\) is \\(\\mathbf{P}\\)-absolutely continuous, with density function \\(f_Y.\\) Use the Inverse Function Theorem to show that \\[ \\frac{\\mathrm{d}}{\\mathrm{d}\\tau}Q_Y(\\tau) = \\frac{1}{f_Y(Q_Y(\\tau))}, \\tau\\in(0,1) \\] (make aditional assumptions if necessary). Adopting the convention that \\(\\sup\\{\\tau\\in(0,1)\\colon\\,2&gt;3\\} = 0.\\)↩︎ A partial reciprocal to this result says that whenever \\(F_Y\\) is continuous the random variable \\(F_Y\\circ Y\\) has a uniform distribution on the unit interval.↩︎ This identity is extremely useful as it allows us to bypass Lebesgue’s theory of integration in defining the integral \\(\\mathbf{E}(Y)\\): indeed, the function \\(\\tau\\mapsto Q_Y(\\tau)\\) is monotone, left-continuous and has well defined right limits; thus, \\(Q_Y\\) is Riemann integrable on any compact interval \\([a,b]\\subseteq (0,1).\\) Now it is a matter of calling \\(Y\\) a \\(\\mathbf{P}\\)-integrable function iff \\(\\lim_{a\\downarrow0}\\lim_{b\\uparrow 1}\\int_a^b |Q_Y(\\tau)|\\,\\mathrm{d}\\tau &lt;\\infty\\) and then define \\(\\mathbf{E}(Y):=\\int_0^1 Q_Y(\\tau)\\,\\mathrm{d}\\tau.\\)↩︎ This item shows that the “correct” loss function to consider should be \\(L(y) = \\rho_\\tau(Y - y) - \\rho_\\tau(Y)\\) and not \\(L(y) = \\rho_\\tau(Y-y)\\) as one usually sees in the literature.↩︎ "],["conditioning-the-plot-thickens.html", "3 Conditioning: the plot thickens", " 3 Conditioning: the plot thickens We have seen that probability distributions (or cumulative distribution functions, or yet quantile functions) are a fundamental tool in modeling, describing and quantifying uncertainty about one-dimensional numerical phenomena. Unfortunately the univariate setting is too restrictive: a handful of applications in Statistics and Machine Learning (and many other Scientific fields) deal with a scenario where we have uncertainty “up to covariates,” meaning that the value of a given variable (possibly a vector) of interest (the response variable) is partially determined by the values of other variables (the covariates or regressors), the latter being known to the researcher (and maybe even under her control). Regression, in a myriad of guises, appears as one of the methods most widely employed to model the dependence structure between a response and covariates. Before moving on to regression proper, let us first introduce a precise notion of conditioning. In the statement below, \\(\\mathbb I\\) denotes the indicator function. Theorem 3.1 (Regular conditional distribution) Let \\(Y\\) be a real valued random variable and let \\(X\\) be a \\({\\mathrm{D}_X}\\)-dimensional random vector. Then there exists a function \\((B,x)\\mapsto \\pi(B,x)\\) defined for Borel sets \\(B\\subseteq\\mathbb{R}\\) and \\(x\\in \\mathbb{R}^{\\mathrm{D}_X},\\) satisfying the following: for each fixed \\(x\\in\\mathbb{R}^{\\mathrm{D}_X},\\) the function \\(B\\mapsto \\pi(B,x)\\) is a Borel probability measure on \\(\\mathbb{R}.\\) for each fixed Borel subset \\(B\\subseteq\\mathbb{R},\\) the function \\(x\\mapsto \\pi(B,x)\\) is measurable and integrable. It holds that \\[\\begin{equation} \\mathbf{P}[Y\\in B, X\\in A] = \\mathbf{E}\\big[\\pi(B,X)\\cdot\\mathbb{I}[X\\in A]\\big] \\tag{3.1} \\end{equation}\\] Moreover, the function \\(\\pi\\) above is essentially unique in the sense that, if \\(\\pi_0\\) is another function satisfying items 1 to 3, then there exists a Borel subset \\(A^*\\subseteq\\mathbb{R}^{\\mathrm{D}_X}\\) with \\(\\mathbf{P}[X\\in A^*]=1\\) such that \\(\\pi(B,x) = \\pi_0(B,x)\\) for all Borel subset \\(B\\subseteq\\mathbb{R}\\) and all \\(x\\in A^*.\\) Remark. Theorem 3.1 still holds when \\(Y\\) is a random vector. We chose to state it in the simpler setting of a scalar \\(Y\\) since this is the case which will be more prevalent throughout the text. Also, the “essential uniqueness” of \\(\\pi\\) tells us in particular that the definition of \\(\\pi(\\cdot,x)\\) for \\(x\\) outside the support of \\(X\\) is arbitrary. The function \\(\\pi\\) appearing in Theorem 3.1 is called the regular conditional distribution of \\(Y\\) given \\(X\\). It is customary to write \\(\\pi(B,x) =: \\mathbf{P}[Y\\in B\\,|\\,X=x]\\) and call this right hand side the conditional probability of the event \\([Y\\in B]\\) given \\(X=x\\). It is also convenient to use the notation \\(\\pi(B,X) =: \\mathbf{P}[Y\\in B\\,|\\, X]\\) (this is a random variable) which is called the conditional probability of the event \\([Y\\in B]\\) given \\(X\\). The difference is subtle but relevant. Notice also that there is redundancy in writing \\(\\mathbf{P}[Y\\in B\\,|\\,X=x]\\): for if \\(\\mathbf{P}[X=x]&gt;0\\) for some \\(x,\\) the elementary notion of conditional probability would entreat us to think of the equality \\[\\begin{equation} \\mathbf{P}[Y\\in B\\,|\\,X=x] = \\frac{\\mathbf{P}[Y\\in A, X=x]}{\\mathbf{P}[X=x]}, \\end{equation}\\] but \\(\\pi(B,x)\\) is not necessarily expressible as above. However, when \\(X\\) is discrete, this is precisely the case, as the following exemple illustrates. Example 3.1 Let \\(Y\\) be a random variable and \\(X\\) a \\(\\mathbf{P}\\)-discrete random vector of dimension \\({\\mathrm{D}_X}.\\) Then the function \\(\\pi\\) defined, for Borel subsets \\(B\\subseteq\\mathbb{R}\\) and \\(x\\in\\mathbb{R}^{\\mathrm{D}_X},\\) by the relation \\[\\begin{equation} \\pi(B,x)\\mathbf{P}[X=x] := \\mathbf{P}[Y\\in B, X=x] \\end{equation}\\] is a regular conditional distribution of \\(Y\\) given \\(X\\) (it is an exercise to check that this assertion is true!) Example 3.2 Let \\(Y\\) be a scalar random variable, and let \\(X\\) be a \\({\\mathrm{D}_X}\\)-dimensional random vector. Assume that \\((Y,X)\\) is \\(\\mathbf{P}\\)-absolutely continuous, meaning that \\((Y,X)\\) have joint density function \\(f_{Y,X}.\\)7 Then the function \\(\\pi\\) defined, for Borel subsets \\(B\\subseteq\\mathbb{R}\\) and \\(x\\in\\mathbb{R}^{\\mathrm{D}_X},\\) by \\[\\begin{equation} \\pi(B,x) := \\int_{B} f_{Y|X}(y|x)\\,\\mathrm{d}y, \\end{equation}\\] is a regular conditional distribution of \\(Y\\) given \\(X.\\) In the above, \\(f_X\\) is the marginal density function of \\(X\\), i.e., \\(f_X(x) = \\int_\\mathbb{R}f_{Y,X}(y,x)\\,\\mathrm{d}y,\\) and \\(f_{Y|X}\\) is the conditional density function of \\(Y\\) given \\(X\\), defined by the relation \\[\\begin{equation} f_{Y|X}(y|x)f_X(x) := f_{Y,X}(y,x) \\end{equation}\\] for \\(x\\in \\mathbb{R}^{\\mathrm{D}_X}\\) and \\(y\\in\\mathbb{R}.\\) \\(\\blacksquare\\) It is important to notice that the equality in (3.1) can be rewritten as \\[\\begin{equation} \\mathbf{P}[Y\\in B, X\\in A] = \\int_A \\mathbf{P}(Y\\in B\\,|\\,X=x)\\,F_X(\\mathrm{d}x), \\tag{3.2} \\end{equation}\\] where “\\(\\int \\cdot \\,F_X(\\mathrm{d}x)\\)” is used to denote the Lebesgue-Stieltjes integral (\\(F_X\\) being the cumulative distribution function of \\(X\\)). Of course, whenever \\(X\\) is \\(\\mathbf{P}\\)-absolutely continuous, with density function \\(f_X,\\) this integral reduces to \\[\\begin{equation} \\mathbf{P}[Y\\in B, X\\in A] = \\int_A \\mathbf{P}(Y\\in B\\,|\\,X=x)f_X(x)\\,\\mathrm{d}x. \\end{equation}\\] Similarly, if \\(X\\) is discrete with probability mass function \\(p_X,\\) we have \\[\\begin{equation} \\mathbf{P}[Y\\in B, X\\in A] = \\sum_{\\{x\\in A\\,:\\,p_X(x)&gt;0\\}}\\mathbf{P}(Y\\in B\\,|\\,X=x) p_X(x) \\end{equation}\\] which, in view of example 3.1, is just the classical Law of Total Probability. The essentially unique non-negative function satisfying the identity \\(F_{Y,X}(y,x) = \\int_{-\\infty}^y\\int_{-\\infty}^x f_{Y,X}(u,v)\\,\\mathrm{d}v\\mathrm{d}u\\) for all \\(y\\in\\mathbb{R}\\) and \\(x\\in \\mathbb{R}^{\\mathrm{D}_X}.\\) Here, \\(\\int_{-\\infty}^x \\mathrm{d}v\\) means \\(\\int_{-\\infty}^{x_1}\\cdots \\int_{-\\infty}^{x_{\\mathrm{D}_X}} \\mathrm{d}v_{\\mathrm{D}_X}\\cdots \\mathrm{d}v_1.\\)↩︎ "],["the-substitution-principle.html", "3.1 The Substitution Principle", " 3.1 The Substitution Principle The random variable \\(Y\\) above is quite arbitrary: in particular, it may be of the form \\(Y = \\varphi(X,Z),\\) where \\(Z\\) is a random vector and where \\(\\varphi\\) is a given measurable function. If this is the case, one feels tempted to ask: “conditional on \\(X=x,\\) does \\(X\\) behave as a constant?” The answer is affirmative: Theorem 3.2 Let \\(X\\) and \\(Z\\) be random vectors of dimensions \\({\\mathrm{D}_X}\\) and \\({\\mathrm{D}_Z}\\) respectively. If \\(\\varphi\\colon\\mathbb{R}^{{\\mathrm{D}_X}}\\times\\mathbb{R}^{{\\mathrm{D}_Z}}\\to\\mathbb{R}\\) is a measurable function, then there exists a Borel set \\(A^*\\subseteq\\mathbb{R}^{\\mathrm{D}_X}\\) such that \\[\\begin{equation} \\mathbf{P}[\\varphi(X,Z)\\in B\\,|\\, X=x] = \\mathbf{P}[\\varphi(x,Z)\\in B\\,|\\, X=x] \\end{equation}\\] for all Borel sets \\(B\\subseteq \\mathbb{R}\\) and all \\(x\\in A^*.\\) Remark. The equality stated in the above theorem actually means that, for each Borel set \\(B\\subseteq \\mathbb{R},\\) one has \\[ \\int \\mathbf{P}[\\varphi(X,Z)\\in B\\,|\\,X=x]\\,F_X(\\mathrm{d}x) = \\int \\mathbf{P}[Z\\in B_x\\,|\\,X=x]\\,F_X(\\mathrm{d}x), \\] where, for each \\(x\\in\\mathbb{R}^{\\mathrm{D}_X},\\) \\(B_x := \\{z\\in \\mathbb{R}^{{\\mathrm{D}_Z}}\\colon\\,\\varphi(x,z)\\in B\\}.\\) Example 3.3 With the same notation as in the above theorem, assume the function \\(\\varphi\\) does not depend on its second argument, that is, for some \\(\\psi\\colon\\mathbb{R}^{\\mathrm{D}_X}\\to\\mathbb{R}\\) it holds that \\(\\varphi(x,z) = \\psi(x)\\) for all \\(x\\in\\mathbb{R}^{\\mathrm{D}_X}\\) and \\(z\\in\\mathbb{R}^{{\\mathrm{D}_Z}}.\\) Then an easy check tells us that \\[\\begin{equation} \\mathbf{P}[\\psi(X)\\in B\\,|\\,X=x] = \\begin{cases} 0,&amp; \\textsf{if } \\psi(x)\\notin B\\\\ 1,&amp; \\textsf{if } \\psi(x)\\in B \\end{cases} \\end{equation}\\] for any Borel set \\(B\\subseteq\\mathbb{R}.\\) That is, \\(\\mathbf{P}[\\psi(X)\\in B\\,|\\,X=x] = \\mathbb{I}[\\psi(x)\\in B].\\) In particular, \\(\\mathbf{P}[\\psi(X) = \\psi(x)\\,|\\,X=x] = 1\\) (not surprisingly). The above also is true with \\(\\psi = \\hphantom{\\!}\\) “the identity function” (valid when \\({\\mathrm{D}_X}=1\\) or by generalizing Theorem 3.1 to vector-valued \\(Y.\\)) Exercise 3.1 Show that, if \\(Y\\) is independent from \\(X,\\) then \\(\\mathbf{P}[Y\\in B\\,|\\,X=x] = \\mathbf{P}[Y\\in B]\\) for all admissible \\(B\\subseteq\\mathbb{R}\\) and all \\(x\\in\\mathbb{R}^{\\mathrm{D}_X},\\) that is, one can take \\(\\pi(B,x) = \\mathbf{P}[Y\\in B]\\) in Theorem 3.1. \\(\\blacksquare\\) "],["conditional-cdfs-and-quantile-functions.html", "3.2 Conditional CDFs and Quantile functions", " 3.2 Conditional CDFs and Quantile functions In section 2 we have put forth the case that univariate probability distributions are essentially the same thing as their corresponding cumulative distribution functions, which in turn are essentially the same thing as their quantile functions. Well, whenever \\(Y\\) is a scalar random variable (and \\(X\\) is a random vector), for a fixed \\(x\\in\\mathbb{R}^{\\mathrm{D}_X}\\) the real valued function \\[\\begin{equation} B\\mapsto \\mathbf{P}[Y\\in B\\,|\\, X=x] \\tag{3.3} \\end{equation}\\] is in fact a probability distribution on the Borel subsets of \\(\\mathbb{R}.\\) Thus, it not only makes sense to define the corresponding cumulative distribution function, but it is also the case that the latter function fully characterizes the mapping in equation (3.3). The conditional cumulative distribution of \\(Y\\) given \\(X\\) is the function \\((y,x)\\mapsto F_{Y|X}(y|x)\\) defined on \\(\\mathbb{R}\\times\\mathbb{R}^{\\mathrm{D}_X}\\) by \\[\\begin{equation} F_{Y|X}(y|x) := \\mathbf{P}[Y\\le y\\,|\\,X=x]. \\end{equation}\\] Importantly, the machinery introduced so far allows us to write the joint cumulative distribution function of \\((Y,X)\\) as8 \\[\\begin{equation} F_{Y,X}(y,x) = \\int_{-\\infty}^x F_{Y|X}(y|u)\\,F_X(\\mathrm{d}u),\\quad y\\in\\mathbb{R}, x\\in\\mathbb{R}^{\\mathrm{D}_X}. \\tag{3.4} \\end{equation}\\] Exercise 3.2 Find an expression for \\(F_{Y|X}(y|x)\\): in terms of the joint density function \\(f_{Y,X},\\) assuming \\((Y,X)\\) is absolutely continuous. in terms of the joint mass function \\(p_{Y,X},\\) assuming \\((Y,X)\\) is discrete. \\(\\blacksquare\\) But this is a text about quantile regression, and now comes a fundamental definition in this direction: in the same setting as above, we let the conditional quantile function of \\(Y\\) given \\(X\\) be the function \\((\\tau,x)\\mapsto Q_{Y|X}(\\tau|x)\\) defined on \\((0,1)\\times\\mathbb{R}^{\\mathrm{D}_X}\\) by \\[\\begin{equation} Q_{Y|X}(\\tau|x) = \\inf\\{y\\in\\mathbb{R}\\colon\\, F_{Y|X}(y|x)\\ge\\tau\\}. \\end{equation}\\] Wrapping up what we have seen so far, the catch is that a probability distribution on \\(\\mathbb{R}\\times\\mathbb{R}^{\\mathrm{D}_X}\\) is entirely determined by the marginal distribution of \\(X\\) together with the conditional quantile function \\(Q_{Y|X},\\) since we can recover the conditional CDF \\(F_{Y|X}\\) from \\(Q_{Y|X}\\) and reconstruct the joint CDF \\(F_{Y,X}\\) via (3.4). From \\(F_{Y,X}\\) we can then compute the probability of any event of the form \\([Y\\in B, X\\in A ]\\) via the formula \\[ \\mathbf{P}[Y\\in B, X\\in A] = \\int_{B\\times A}\\,F_{X,Y}(\\mathrm{d}y, \\mathrm{d}x). \\] The idea is not a mere shenanigan: for instance, if \\({\\mathrm{D}_X}=1\\) it provides us a recipe to simulate a pair \\((Y,X)\\) from the distribution \\(\\mathbf{P}_{Y,X}\\) as follows: generate two independent pseudo-random numbers \\(u_1\\) and \\(u_2\\) from the Uniform\\([0,1]\\) distribution. set \\(x = Q_X(u_1)\\) and \\(y = Q_{Y|X}(u_2|x).\\) It follows that the pair \\((y,x)\\) is a pseudo-random draw from \\(\\mathbf{P}_{Y,X}.\\) Theorem 3.3 Assume \\(Y\\) is a scalar random variable, and \\(X\\) is a \\({\\mathrm{D}_X}\\)-dimensional random vector. Denote by \\(F_X\\) the cumulative distribution function of \\(X,\\) and by \\(Q_{Y|X}\\) the conditional quantile function of \\(Y\\) given \\(X.\\) Now let \\(U\\) be a Uniform\\([0,1]\\) random variable independent from \\(X,\\) and define \\(\\tilde{Y} = Q_{Y|X}(U\\,|\\,X).\\) Then \\(\\tilde{Y} \\overset{\\textsf{dist}}= Y.\\) Exercise 3.3 Prove Theorem 3.3. Hint: the bulk of the proof lies in computing \\(\\mathbf{P}[Q_{Y|X}(U|x)\\le y\\,|\\,X=x]\\) and then integrating wrt \\(F_X(\\mathrm{d}x).\\) For those with a Measure Theoretic eye, however, the question of whether the function \\(\\tilde{Y}\\colon\\Omega\\to\\mathbb{R}\\) is measurable may be a deterrent; fortunately, the answer to this question is affirmative — see Theorem 3 in Gowrisankaran (1972). Exercise 3.4 Let \\(Z := g(X) + h(X)Y\\) where \\(g\\) and \\(h\\) are real valued measurable functions on \\(\\mathbb{R}^{\\mathrm{D}_X},\\) \\(h\\) being non-negative. Show that \\(Q_{Z|X}(\\tau|x) = g(x) + h(x)Q_{Y|X}(\\tau|x),\\) for \\(\\tau\\in(0,1).\\) Exercise 3.5 For \\(\\tau\\in(0,1),\\) let \\(g_\\tau\\colon\\mathbb{R}^{\\mathrm{D}_X}\\to\\mathbb{R}\\) be a measurable function. Show that \\[\\begin{equation} \\mathbf{E}\\rho_\\tau\\big(Y - Q_{Y|X}(\\tau|x)\\big) \\le \\mathbf{E}\\rho_\\tau\\big(Y - g_\\tau(X)\\big). \\end{equation}\\] Show that, if the mapping \\((\\tau,x)\\mapsto g_\\tau(x)\\) is measurable, then \\[\\begin{equation} \\int_0^1\\mathbf{E}\\rho_\\tau\\big(Y - Q_{Y|X}(\\tau|x)\\big)\\,\\mathrm{d}\\tau \\le \\int_0^1\\mathbf{E}\\rho_\\tau\\big(Y - g_\\tau(X)\\big)\\,\\mathrm{d}\\tau. \\end{equation}\\] Of course, the integral can be simplified in the cases where \\(X\\) is discrete or continuous, as we discussed earlier.↩︎ "],["conditional-expectation.html", "3.3 Conditional Expectation", " 3.3 Conditional Expectation The standard approach in intermediary and advanced Probability textbooks is to introduce the conditional expectation of \\(Y\\) given \\(X\\) (this is defined for integrable \\(Y\\)) as any random variable \\(W\\) that satisfies the following two conditions: There exists a measurable function \\(\\varphi\\colon\\mathbb{R}^{\\mathrm{D}_X}\\to\\mathbb{R}\\) such that \\(W = \\varphi(X).\\) It holds that \\(\\mathbf{E}(Y\\,\\mathbb{I}[X\\in A]) = \\mathbf{E}(W\\,\\mathbb{I}[X\\in A]\\)), for all Borel subsets \\(A\\subseteq\\mathbb{R}^{\\mathrm{D}_X}.\\) The proof that one can always find such a \\(W\\) relies on the famous Radon-Nikodym Theorem from Measure Theory, combined with the Doob-Dynkin Lemma which provides us with the function \\(\\varphi.\\) As a matter of fact, one can obtain the regular conditional distribution in 3.1 as a corollary to this fact (a direct proof also relies on the Radon-Nikodym Theorem, see Billingsley (1995), Section 33, especially Theorem 33.3). Since we introduced the idea of conditioning through the viewpoint of regular conditional distributions, we can take a shortcut and define, for any integrable random variable \\(Y\\) and any random vector9 \\(X,\\) \\[ \\mathbf{E}(Y\\,|\\,X = x) := \\int_0^1 Q_{Y|X}(\\tau|x)\\,\\mathrm{d}\\tau,\\qquad x\\in\\mathbb{R}^{\\mathrm{D}_X}, \\] called the conditional expectation of \\(Y\\) given \\(X=x\\), and, writing \\(\\varphi(x) := \\mathbf{E}(Y\\,|\\,X=x),\\) let \\[ \\mathbf{E}(Y\\,|\\,X) := \\varphi(X), \\] called the conditional expectation of \\(Y\\) given \\(X\\). Exercise 3.6 Show that \\(\\mathbf{E}(\\mathbb{I}[Y\\in B]\\,|\\,X=x) = \\mathbf{P}[Y\\in B\\,|\\,X=x]\\) holds for every \\(x\\in\\mathbb{R}^{\\mathrm{D}_X}\\) and all Borel sets \\(B\\subseteq \\mathbb{R}.\\) \\(\\mathbf{E}\\{\\mathbf{E}(Y|X)\\,\\mathbb{I}[X\\in A]\\} = \\mathbf{E}(Y\\,\\mathbb{I}[X\\in A])\\) holds for all Borel sets \\(A\\subseteq\\mathbb{R}^{\\mathrm{D}_X}.\\) \\(\\mathbf{E}(aY + Z\\,|\\,X) = a\\mathbf{E}(Y\\,|\\,X) + \\mathbf{E}(Y\\,|\\,X)\\) for any \\(a\\in\\mathbb{R}.\\) (Substitution principle) \\(\\mathbf{E}(\\psi(X,Z)\\,|\\,X=x) = \\mathbf{E}(\\psi(x,Z)\\,|\\,X=x).\\) In particular, \\(\\mathbf{E}(\\psi(X)Y\\,|\\,X=x) = \\psi(x)\\mathbf{E}(Y\\,|\\,X=x)\\) and \\(\\mathbf{E}(\\psi(X)Y\\,|\\,X) = \\psi(X)\\mathbf{E}(Y\\,|\\,X).\\) If \\(Y\\) and \\(X\\) are mutually independent, then \\(\\mathbf{E}(Y\\,|\\,X) = \\mathbf{E}(Y).\\) This holds, in particular, if \\(X\\) is constant. In fact, the definition is still valid even if \\(X\\) is an infinite sequence of random variables. This fact will be useful in the context of time series quantile regression models, where we typically condition on the entire “past” \\(\\{(Y_{s},X_{s}):s&lt;t\\}.\\)↩︎ "],["quantile-regression.html", "4 Quantile Regression", " 4 Quantile Regression Quantile Regression surfaced in its modern guise in the seminal Econometrica paper by Roger Koenker and Gilbert Bassett (Koenker and Bassett 1978). Similarly to (parametric) mean regression models, where the conditional expectation of the response given the covariates is a linear function of the latter, in its most basic form the quantile regression framework stipulates that the conditional quantile function of a scalar random variable \\(Y\\) given a \\({\\mathrm{D}_X}\\)-dimensional random vector \\(X\\) is a linear function of these covariates: the cornerstone assumption is that, for \\(\\tau\\in \\mathscr{T}\\subseteq(0,1)\\) and \\(x\\in\\operatorname{support}(X)=:\\mathscr{X},\\)10 the representation \\[\\begin{equation} Q_{Y|X}(\\tau\\,|\\,x) = \\sum_{d=1}^{\\mathrm{D}_X}\\beta_d(\\tau) x_{d} \\equiv x&#39;\\beta(\\tau), \\tag{4.1} \\end{equation}\\] holds for some functional parameter \\(\\beta\\colon\\mathscr{T}\\to\\mathbb{R}^{\\mathrm{D}_X}.\\) When \\(\\mathscr{T} = (0,1),\\) Zheng, Peng, and He (2015) call the model in equation (4.1) a globally concerned quantile regression model.11 In contrast, when \\(\\mathscr{T}\\) is a countable set (in particular when it is a singleton), they call (4.1) a locally concerned quantile regression model. Of course, as in classical regression one needs to assume that the random variables \\(X_1,\\dots,X_{\\mathrm{D}_X}\\) are linearly independent12 so that the parameters in (4.1) are identified. Example 4.1 If \\(\\mathscr{T}=\\{^1\\!/\\!_2\\},\\) then (4.1) is a median regression model and, putting \\(\\alpha:=\\beta(^1\\!/\\!_2),\\) we can write \\[ Y = X^\\prime \\alpha + \\varepsilon \\] with \\(Q_{\\varepsilon | X}(^1\\!/\\!_2|x) = 0\\) for all \\(x\\in \\mathbb{R}^{\\mathrm{D}_X}.\\) In fact, for a locally concerned quantile regression model with \\(\\mathscr{T} = \\{\\tau\\},\\) we can always write \\(Y = X^\\prime \\alpha + \\varepsilon\\) for some vector of parameters \\(\\alpha\\in\\mathbb{R}^{\\mathrm{D}_X}\\) and a random variable \\(\\varepsilon\\) satisfying \\(Q_{\\varepsilon | X}(\\tau|x) = 0.\\) \\(\\blacksquare\\) The situation depicted in the above example has many useful applications. For instance, median regression allows one to obtain a robust point forecast \\(x&#39;\\widehat{\\alpha}\\) for the response when the conditional distribution of \\(Y\\) given \\(X\\) is heavy-tailed (\\(Y\\) can even fail to be integrable). Notwithstanding, a locally concerned quantile regression model fails to take full advantage of the fact that (conditional) quantile functions completely characterize the (conditional) distributions. Therefore, in what follows I’ll always have in mind the globally concerned quantile regression model (4.1) with \\(\\mathscr{T} = (0,1).\\) With this, as we have argued earlier, the joint distribution of \\(X\\) and \\(Y\\) is entirely encoded in the marginal distribution of \\(X\\) and the functional parameter \\(\\beta:\\) it holds that \\[\\begin{align} \\begin{split} \\mathbf{P}[Y\\in B, X\\in A] &amp;= \\int_{A}\\int_0^1 \\mathbb{I}[x&#39;\\beta(\\tau)\\in B]\\,\\mathrm{d}\\tau\\,F_X(\\mathrm{d}x)\\\\ &amp;= \\int_0^1 \\mathbf{P}[X&#39;\\beta(\\tau)\\in B, X\\in A]\\,\\mathrm{d}\\tau \\end{split} \\tag{4.2} \\end{align}\\] for every pair of Borel sets \\(B\\subseteq \\mathbb{R}\\) and \\(A\\subseteq \\mathbb{R}^{\\mathrm{D}_X}.\\) Nevertheless, in regression models one is usually uninterested in the distribution of the covariates: rather, such models aim to quantify uncertainty about \\(Y\\) given \\(X.\\) Mean regression describes this uncertainty in terms of the conditional expected value of \\(Y\\) given \\(X\\); median regression, in terms of the conditional median of \\(Y,\\) and so on. The globally concerned quantile regression model, in turn, quantifies uncertainty by specifying the entire conditional distributions of \\(Y\\) given \\(X\\) (of course, via the corresponding conditional quantile functions). In this aspect, it is not too different from fully parametric generalized linear models, which also specify the conditional distribution of the response given the predictors. Quantile regression can be seen as a different take on how to achieve said specification: indeed, the model (4.1) could be described as non-parametric, since the parameter of interest is infinite dimensional, although the functional form of \\(Q_{Y|X}\\) is partially parametrized/constrained by the “linearity in \\(x\\)” assumption. At any rate, and this is not obvious at first sight, the fact is that a conditional quantile function of the form (4.1) permits a very flexible structure of dependence between \\(Y\\) and \\(X,\\) allowing the covariates to modify not only the mean but also the variance, the coefficient of asymmetry, the number of modes, etc, of the response. To sum up, the quantile regression model is a flexible and parsimonious way to specify the structure of dependence between the response and covariates. As put forth by Koenker (2005), An attractive feature of quantile regression that has been repeatedly emphasized is that it enables us to look at slices of the conditional distribution without any reliance on global distributional assumptions. Another important property of the quantile regression model (4.1) is that the parameter \\(\\beta\\) appears as a solution to an optimization problem. This will be relevant later on when we are dealing with estimation. Theorem 4.1 If equation (4.1) holds for all \\(\\tau\\in\\mathscr{T}\\subseteq(0,1)\\) and all \\(x\\in\\mathscr{X},\\) then: \\(\\mathbf{E}\\rho_\\tau\\big(Y - X^\\prime \\beta(\\tau)\\big) \\le \\mathbf{E}\\rho_\\tau\\big(Y - X^\\prime b\\big)\\) for any \\(b\\in\\mathbb{R}^{\\mathrm{D}_X}\\) and \\(\\tau\\in\\mathscr{T}.\\) \\(\\int_{\\mathscr{T}}\\mathbf{E}\\rho_\\tau\\big(Y - X^\\prime \\beta(\\tau)\\big)\\,\\mathrm{d}\\tau \\le \\int_{\\mathscr{T}}\\mathbf{E}\\rho_\\tau\\big(Y - X^\\prime b(\\tau)\\big)\\,\\mathrm{d}\\tau\\) for any measurable function \\(b\\colon(0,1)\\to\\mathbb{R}^{\\mathrm{D}_X},\\) provided \\(\\mathscr{T}\\) is a Borel set (this is the case, in particular, if \\(\\mathscr{T}=(0,1)\\)). If \\(\\mathbf{B}\\) is any \\({\\mathrm{D}_X}\\times \\mathrm{M}\\) matrix with \\(d\\)th column \\(b_m\\in\\mathbb{R}^{\\mathrm{D}_X}\\), and if \\(0&lt;\\tau_1&lt;\\cdots&lt;\\tau_{\\mathrm{M}}&lt;1,\\) then \\(\\sum_{m=1}^\\mathrm{M}\\mathbf{E}\\rho_{\\tau_m}\\big(Y - X^\\prime\\beta(\\tau_m)\\big)\\le \\sum_{m=1}^\\mathrm{M}\\mathbf{E}\\rho_{\\tau_m}\\big(Y - X^\\prime b_m\\big).\\) Proof. Write \\(Q = Q_{Y|X}\\) for simplicity, so \\(Q(\\tau|x) = x^\\prime\\beta(\\tau).\\) For item 1, from the univariate setting we know that, given any \\(x\\in\\mathscr{X},\\) one has \\[ \\mathbf{E}\\big\\{\\rho_\\tau\\big(Y - Q(\\tau|x)\\big)\\,|\\,X = x\\big\\} \\le \\mathbf{E}\\big\\{\\rho_\\tau\\big(Y - y\\,|\\,X = x\\big)\\,|\\,X=x\\big\\} \\] for all \\(y\\in \\mathbb{R},\\) and this is true in particular when \\(y\\) is of the form \\(y = x^\\prime b\\) for some \\(b\\in\\mathbb{R}^{\\mathrm{D}_X}.\\) Thus, by iterated expactations, monotonicity of the Riemann-Stieltjes integral and the substitution principle, \\[\\begin{align} \\mathbf{E}\\rho_\\tau(Y - X^\\prime\\beta(\\tau)) &amp;= \\int \\mathbf{E}\\big\\{\\rho_\\tau\\big(Y - x^\\prime\\beta(\\tau)\\big)\\,|\\,X = x\\big\\}\\, F_X(\\mathrm{d}x)\\\\ &amp;\\le \\int \\mathbf{E}\\big\\{\\rho_\\tau\\big(Y - x^\\prime b\\big)\\,|\\,X = x\\big\\}\\, F_X(\\mathrm{d}x)\\\\ &amp; =\\mathbf{E}\\rho_\\tau(Y - X^\\prime b) \\end{align}\\] The second item is just a matter of noticing that, if \\(b\\colon(0,1)\\to\\mathbb{R}^{\\mathrm{D}_X}\\) is any measurable function, then item 1 ensures that \\(\\mathbf{E}\\rho_\\tau\\big(Y - X^\\prime \\beta(\\tau)\\big)\\) is bounded above by \\(\\mathbf{E}\\rho_\\tau\\big(Y - X^\\prime b(\\tau)\\big)\\) for all \\(\\tau\\in (0,1).\\) Thus, the asserted inequality follows, again by monotonicity of the Riemann-Stieltjes integral. The third item follows by a similar argument. Exercise 4.1 Prove equation (4.2). The support of \\(X,\\) which we shall denote by \\(\\mathscr{X},\\) is defined by the following two conditions: 1. \\(\\mathscr{X}\\) is a closed subset of \\(\\mathbb{R}^{\\mathrm{D}_X}\\) satisfying \\(\\mathbf{P}[X\\in\\mathscr{X}]=1,\\) and; 2. If \\(A\\subseteq\\mathbb{R}^{\\mathrm{D}_X}\\) is closed and \\(\\mathbf{P}[X\\in A] = 1,\\) then \\(A\\supseteq\\mathscr{X}.\\) That is \\(\\mathscr{X}\\) is the smallest closed set in \\(\\mathbb{R}^{\\mathrm{D}_X}\\) having total \\(\\mathbf{P}_X\\) probability.↩︎ Actually, they call the model globally concerned also in the case when \\(\\mathscr{T}\\) is an interval.↩︎ This means that that the event \\[\\left\\{\\omega\\in\\Omega : x^\\prime X(\\omega)=0\\ \\textsf{for some non-zero}\\ x\\in\\mathbb{R}^{\\mathrm{D}_X}\\right\\}\\] has null \\(\\mathbf{P}\\)-probability.↩︎ "],["drawbacks.html", "4.1 Drawbacks", " 4.1 Drawbacks Flexibility comes at a cost, however. When we write equation (4.1), we are implicitly restricting either the functional form of \\(\\beta,\\) or the support of \\(X,\\) or both, because the map \\(\\tau\\mapsto Q_{Y|X}(\\tau|x)\\) is non-decreasing and left-continuous, for all \\(x\\in\\mathscr{X}.\\) In this section I’ll discuss some of these restrictions; for conciseness, I will not repeat at every turn that \\(Y\\) is a scalar random variable, that \\(X\\) is a \\({\\mathrm{D}_X}\\)-dimensional random vector, and that \\(Y\\) and \\(X\\) are related by \\[\\begin{equation} Q_{Y|X}(\\tau|x) = x&#39;\\beta(\\tau),\\qquad \\tau\\in(0,1),\\,x\\in\\mathscr{X}. \\tag{4.3} \\end{equation}\\] This is our working assumption. The first issue, illustrated in the example below, is called quantile crossing, which we couldn’t explain better than Koenker (2005) already has: The virtues of independently estimating a family of conditional quantile functions can sometimes be a source of serious embarrassment when we find that estimated quantile functions cross, thus violating the basic principle that distribution functions and their associated inverse functions should be monotone increasing. […] It is of some comfort to recognize that such crossing is typically confined to outlying regions of the design space. Example 4.2 (Quantile crossing) Suppose that \\(X = (1\\quad X_2)^\\prime,\\) where \\(X_2\\) is real valued. If \\(\\beta_2(\\cdot)\\) is a non-constant function, then the support of \\(X_2\\) has a either a lower bound or an upper bound. Thus, if \\(X_2\\) is an unbounded random variable, it is necessarily the case that \\(\\beta_2(\\cdot)\\) is a constant function. Indeed, if the coefficient \\(\\beta_2\\) is not constant-in-\\(\\tau,\\) then there exist two distinct quantile levels \\(\\tau&lt;\\varsigma\\in(0,1)\\) such that either \\(\\beta_2(\\tau)&lt;\\beta_2(\\varsigma)\\) or \\(\\beta_2(\\tau)&gt;\\beta_2(\\varsigma).\\) In any case, if \\(\\operatorname{support}(X_2)\\) were unbounded from above and below, then it would be possible to find an \\(x_2\\in\\operatorname{support}(X_2)\\) such that, for \\(x = (1\\quad x_2)^\\prime,\\) one would have \\(Q_Y(\\tau|x)&gt;Q_Y(\\varsigma|x)\\) which is forbidden since \\(\\tau\\mapsto Q_Y(\\tau|z)\\) is non-decreasing. This example easily generalizes to the scenario where \\(X\\) is of dimension \\({\\mathrm{D}_X}&gt; 2\\): if a covariate is unbounded and can “move freely,” independently of the remaining regressors, then it must have a constant-in-\\(\\tau\\) coefficient. Notice, however, that this restriction does not necessarily apply: indeed, equation (4.3) can hold exactly (no crossing), provided there is “sufficient dependence” between the covariates, even if they are unbounded. As an example, take \\(X = (1\\quad Z\\quad Z^2)^\\prime\\) with \\(Z\\) standard normal (so \\(\\operatorname{support}(Z)=\\mathbb{R}\\)) and \\[ \\beta(\\tau) = \\begin{pmatrix} \\tfrac12Q_Z(\\tau) \\\\ 1\\\\ 2\\tau-1\\end{pmatrix} \\] for all \\(\\tau\\in(0,1).\\) Below is a scatterplot of simulated data from this model. The dashed blue lines correspond (from bottom to top) to the conditional 1st decile, median, and 9th decile. For more about quantile crossing, see section 2.5 in Koenker (2005). n = 800 Q = function(tau,z) 1*qnorm(tau)/2 + 1*z + (2*tau-1)*(z)^(2) Z = rnorm(n) U = runif(n) Y = Q(U,Z) plot(Y~Z, pch=16, col=&quot;gray&quot;) zgrid = seq(from=min(Z), to=max(Z), length=n) for (tau in c(.1,.5,.9)){ lines(zgrid, Q(tau,zgrid), lty=&quot;dashed&quot;, col=&quot;DarkBlue&quot;, lwd=.5) } Example 4.3 (Sign restrictions) Assume that \\(X\\) is of dimension \\({\\mathrm{D}_X}=2\\) and that \\(X_1\\) and \\(X_2\\) are non-degenerate (there’s no intercept). Suppose further that there are two distinct points \\(\\widehat{x}, \\tilde{x}\\in \\mathscr{X}\\) with \\(\\operatorname{sign}(\\widehat{x}_1)\\ne\\operatorname{sign}(\\tilde{x}_1)\\) and \\(\\widehat{x}_2=\\tilde{x}_2=0.\\) Then necessarily \\(\\beta_1\\) is constant-in-\\(\\tau\\): indeed, in this case the functions \\(\\tau\\mapsto \\beta_1(\\tau)\\hat{x}_1\\) and \\(\\tau\\mapsto\\beta_1(\\tau)\\tilde{x}_1\\) are both non-decreasing (being conditional quantile functions) and thus \\[ \\tau\\mapsto\\mathrm{sign}(\\hat{z}_1)\\beta_1(\\tau)\\qquad\\text{and}\\qquad\\tau\\mapsto\\mathrm{sign}(\\tilde{z}_1)\\beta_1(\\tau) \\] are both non-decreasing functions, which can only happen if \\(\\beta_1\\) is constant-in-\\(\\tau.\\) This example can be generalized to the case \\({\\mathrm{D}_X}&gt;2.\\) Example 4.4 (Classical linear regression) Let \\(Z\\) be a scalar Gaussian random variable with \\(\\mathbf{E}(Z)=\\mu\\) and \\(\\operatorname{Var}(Z)=\\sigma_X^2.\\) Assume \\[ Y = a + bZ + U \\] for some real constants \\(a\\) and \\(b\\) and some random variable \\(U\\sim N(0,\\sigma^2)\\) independent of \\(Z.\\) Then \\[ Q_{Y|Z}(\\tau|z) = a+bz+Q_U(\\tau) \\] for all \\(\\tau\\in(0,1)\\) and \\(z\\in\\mathbb{R}\\) (check!), which yields the quantile regression model (4.3) with \\(X = (1\\quad Z)^\\prime,\\) \\(\\beta_1(\\cdot) = a + Q_U(\\cdot)\\) and \\(\\beta_2(\\cdot) = b.\\) Here there is no crossing, as the coefficient \\(\\beta_2\\) is a constant function. Example 4.5 Let \\(Z\\) be a non-negative scalar random variable and assume the polynomial \\(h(z) = 1 + \\gamma_1 z + \\cdots + \\gamma_q z^q,\\) \\(z\\ge0,\\) is non-decreasing. Let \\(U\\) be independent of \\(Z\\) and assume \\[ Y = a + bZ + h(Z)U \\] for some real constants \\(a\\) and \\(b.\\) Then \\[ Q_{Y|Z}(\\tau|z) = a + bz + h(z)Q_U(\\tau) \\] for all \\(\\tau\\in(0,1)\\) and \\(z\\in\\operatorname{support}(Z).\\) Therefore, writing \\(X = (1 \\quad Z \\quad \\cdots\\quad Z^q)^\\prime\\) yields the quantile regression model (4.3) with \\(\\beta_1(\\cdot) = a + Q_U(\\cdot),\\) \\(\\beta_1(\\cdot) = b + \\gamma_1Q_U(\\cdot)\\) and, for \\(d&gt;2,\\) \\(\\beta_d(\\cdot) = \\gamma_d Q_U(\\cdot).\\) Here, \\(Z\\) possibly affects the conditional location and dispertion of \\(Y,\\) but not the other parameters related to the shape of the conditional distribution. \\(\\blacksquare\\) In view of the preceding examples, in the quantile regression model (4.3) it is convenient to restrict attention to covariate vectors whose support is not only bounded but also a bounded subset of \\(\\mathbb{R}^{\\mathrm{D}_X}_+:= [0,+\\infty)^{\\mathrm{D}_X}\\). Notice that imposing such restrictions on the covariates may demand a restriction on the response as well, for example if \\(Y\\) is equal in distribution to some of the regressors (this is the case in stationary time series with an autoregressive component, when \\(X\\) includes lagged values of the response). These restrictions can be relaxed if we allow ourselves a less stringent approach, assuming for instance that the linear specification does not hold exactly but is rather an approximation of the “true model,” valid in a relevant region of the support of \\(X.\\) This can be formalized, for example, by requiring that (4.3) holds (exactly or approximately) not for all \\(x\\in\\mathscr{X}\\) but only for \\(x\\in\\mathscr{X}_0\\) where \\(\\mathscr{X}_0\\subseteq\\mathbb{R}^{\\mathrm{D}_X}\\) is a region with \\(\\mathbf{P}[X\\in\\mathscr{X}_0]&gt; 1-\\epsilon\\) and where \\(\\epsilon&gt;0\\) is a small constant. This “less stringent approach” is convenient when fitting real data, but is unhelpful if one needs to cast a more analytic glance at quantile regression models, or if the aim is to simulate data from (4.3) "],["quantile-regression-via-a-family-of-hyperplanes.html", "4.2 Quantile regression via a family of hyperplanes", " 4.2 Quantile regression via a family of hyperplanes We have seen that, as a way to allow for greater flexibility in the functional form of the parameter \\(\\beta,\\) the covariates in a globally concerned quantile regression model are typically required to lie in a hypercube of the form \\([{m}_1,\\bar{m}_1]\\times\\cdots[{m}_{\\mathrm{D}_X},\\bar{m}_{\\mathrm{D}_X}]\\) for some constants \\(0\\le m_d \\le \\bar{m}_d,\\) \\(d\\in\\{1,\\dots,{\\mathrm{D}_X}\\}.\\) Up to translation and rescaling, we can in fact assume that \\(\\mathscr{X} \\subseteq [0,1]^{\\mathrm{D}_X},\\) and we can even require that \\(0\\) and \\(1\\) lie in the support of each non-constant covariate,13 as the following example clarifies. Example 4.6 Suppose that the quantile regression model (4.3) holds, and that \\(\\operatorname{support}(X_d) \\subseteq [{m}_d,\\bar{m}_d]\\) for some constants \\(0\\le m_d \\le \\bar{m}_d,\\) \\(d\\in\\{2,\\dots,{\\mathrm{D}_X}\\}.\\) We are implicitly assuming that \\({m}_d\\) is the greatest constant for which these inclusions hold, and similarly that \\(\\bar{m}_d\\) is the least constant for which they hold.14 Without loss of generality, we make the assumption that \\(X_1 = 1,\\) as nothing precludes \\(\\beta_1(\\cdot)\\) from being the zero function (this would be the “no intercept” setting). As mentioned earlier, we are also assuming that covariates are linearly independent, and this tells us, in particular, that no covariate other than \\(X_1\\) is constant, which in turn implies \\(\\bar{m}_d &gt; {m}_d\\) for \\(2\\le d\\le{\\mathrm{D}_X}.\\) Now let \\[ \\tilde{X}_d = {(X_d-{m}_d)}/{(\\bar{m}_d - {m}_d)},\\quad 2\\le d\\le{\\mathrm{D}_X}. \\tag{4.4} \\] Writing \\(\\tilde{X} = (1\\quad \\tilde{X}_2\\cdots\\tilde{X}_{\\mathrm{D}_X})^\\prime,\\) and with the slighty technical remark that conditioning on \\(\\tilde{X}\\) is the same as conditioning on \\(X,\\) we then see that the equality \\[ Q_{Y|\\tilde{X}}(\\tau|x) = x^\\prime\\tilde{\\beta}(\\tau) \\] holds for \\(\\tau\\in(0,1)\\) and conformable \\(x\\in\\operatorname{support}(\\tilde{X})\\subseteq \\{1\\}\\times[0,1]^{{\\mathrm{D}_X}-1},\\) where \\(\\tilde{\\beta}\\colon(0,1)\\to\\mathbb{R}^{\\mathrm{D}_X}\\) is defined componentwise through \\[ \\tilde{\\beta}_1 = \\beta_1 + \\sum_{d=1}^{\\mathrm{D}_X}{m}_d\\beta_d\\quad\\textsf{and}\\quad \\tilde{\\beta_d} = (\\bar{m}_d - m_d)\\beta_d,\\,d\\ge2. \\] Exercise 4.2 Prove the validity of each assertion in example 4.6. Fill in with the details where necessary. \\(\\blacksquare\\) So far, I hope that the reader is convinced that the globally concerned quantile regression model allowing an a priori flexible functional form for the parameter \\(\\beta\\) can always be thought of — at least analytically — as a model in which the covariates “include a constant” and are bound to lie in the \\({\\mathrm{D}_X}\\)-dimensional unit cube; in other words, we can always consider \\(\\mathscr{X} \\subseteq \\{1\\}\\times [0,1]^{{\\mathrm{D}_X}-1}.\\) However, we cannot force the latter inclusion to be an equality since this would preclude the scenario where some covariates are functionally related, for example when we have a polynomial in a certain variable. Example 4.7 Assume \\(X = (1\\quad Z\\quad Z^2)^\\prime,\\) where \\(Z\\) is uniformly distributed in the unit interval. Then the support of \\(X\\) is the set \\[ \\mathscr{X} = \\{x\\in\\mathbb{R}^3\\colon\\,x_1 = 1,\\,x_2\\in[0,1],\\,x_3=x_2^2\\} \\] which is a strict subset of \\(\\{1\\}\\times [0,1]^{2}.\\,\\blacksquare\\) In general it is not straightforward to constructively exhibit a non-trivial functional parameter \\(\\tau\\mapsto\\beta(\\tau)\\) for which the mappings \\[ \\tau\\mapsto x^\\prime\\beta(\\tau) \\] are non-decreasing and left-continuous for all \\(x\\in \\mathscr{X},\\) even after “easing up” a little bit to ensure that \\(\\mathscr{X}\\subseteq\\{1\\}\\times[0,1]^{{\\mathrm{D}_X}-1}.\\) In fact, any conditions ensuring that a function \\(\\beta\\colon(0,1)\\to\\mathbb{R}^{{\\mathrm{D}_X}}\\) has the preceding attributes will necessarily be tied to the topology and geometry of \\(\\mathscr{X}.\\) When all the covariates are non-negative, a simple sufficient condition is requiring that each of the coefficients \\(\\tau\\mapsto \\beta_d(\\tau)\\) be non-decreasing. This is somewhat restrictive, however, as in this setting, the quantile regression model (4.3) tells us that the covariates impact mostly the location and dispersion of the response. The example below illustrates this fact.15 Example 4.8 Assume (4.3) holds, with \\(X_1=1\\) and with \\((X_2\\quad X_3)^\\prime\\) uniformly distributed in the unit square, so \\(\\mathscr{X} = \\{1\\}\\times [0,1]\\times [0,1].\\) For \\(\\tau\\in(0,1),\\) let \\(\\beta_1(\\tau) = 3\\tau/4,\\) \\(\\beta_2(\\tau) =\\tau/4+1\\) and \\(\\beta_3(\\tau) = \\tau/4 - 1.\\) Thus, we have for example \\[\\begin{align} Y\\,|\\,(X_2=0,X_3=0) &amp;\\sim \\textsf{Uniform}[0,3/4)\\\\ Y\\,|\\,(X_2=1,X_3=0) &amp;\\sim \\textsf{Uniform}[1,2)\\\\ Y\\,|\\,(X_2=0,X_3=1) &amp;\\sim \\textsf{Uniform}[-1,0)\\\\ Y\\,|\\,(X_2=1,X_3=1) &amp;\\sim \\textsf{Uniform}[0,5/4) \\end{align}\\] and so on. The below figure displays a scatterplot of \\(n\\) points drawn from this model. n = 200 U = runif(n) beta1 = function(tau) 3*tau/4 beta2 = function(tau) tau/4+1 beta3 = function(tau) tau/4-1 X2 = runif(n) X3 = runif(n) Y = beta1(U) + beta2(U)*X2 + beta3(U)*X3 dat = data.frame(Y = Y, X2 = X2, X3 = X3) plot_ly(dat, x = ~X2, y = ~X3, z = ~Y, type=&quot;scatter3d&quot;, mode=&quot;markers&quot;, marker = list(color = ~Y, colorscale = c(&#39;#FFE1A1&#39;, &#39;#683531&#39;) ) ) %&gt;% layout(scene = list(camera = list(eye = list(x = 0, y = -1.5, z = 0)), aspectratio = list(x = 1, y = 1, z = 1/2) ) ) In general, it can be challenging to analytically exhibit coefficient functions \\(\\tau\\mapsto\\beta(\\tau)\\) that yield monotonicity of the linear form \\(\\tau\\mapsto x^\\prime\\beta(\\tau)\\) over a relevant range of \\(x\\)’s. The next result provides a constructive approach to obtain conditional quantile functions with \\(\\mathscr{X} = \\{1\\}\\times [0,1]^{2}.\\) This is particularly useful for Monte Carlo simulation studies. I’m stating the result for the case \\({\\mathrm{D}_X}= 3\\) for simplicity, but a generalization to the case \\({\\mathrm{D}_X}&gt; 3\\) is not too difficult to obtain. Proposition 4.1 Assume that \\(v_{d}\\colon(0,1)\\to\\mathbb{R},\\) \\(d\\in\\{0,1\\}^2,\\) are non-decreasing, left-continuous functions satisfying the requirements \\(v_{11} = v_{10} + v_{01} - v_{00}\\). the mapping \\(\\tau\\mapsto v_{11}(\\tau)\\) is non-decreasing. Let \\(X\\) be a random vector in \\(\\mathbb{R}^3\\) with \\(\\mathscr{X} \\subseteq \\{1\\}\\times[0,1]^2,\\) and let \\(U\\) be a scalar random variable uniformly distributed on the unit interval, independent of \\(X.\\) Then the random variable \\(Y\\) defined via \\[ Y = v_{00}(U) + \\big(v_{10}(U) - v_{00}(U)\\big)X_2 + \\big(v_{01}(U)-v_{00}(U)\\big)X_3 \\] satisfies, for any \\(\\tau\\in(0,1)\\) and all \\(x\\in\\mathscr{X},\\) \\[ Q_{Y|X}(\\tau\\,|\\,x) = \\beta_0(\\tau) + \\beta_1(\\tau)x_2 + \\beta_3(\\tau)x_3 \\] where \\(\\beta_1 = v_{00},\\) \\(\\beta_2 = v_{10} - v_{00}\\) and \\(\\beta_3 = v_{01} - v_{00}.\\) Proof. The proof amounts to showing that the mapping \\[ \\tau\\mapsto v_{00}(\\tau) + (v_{10}(\\tau) - v_{00}(\\tau))x_2 + (v_{01}(\\tau)-v_{00}(\\tau))x_3 \\] is non-decreasing and left-continuous, for any \\((x_2,x_3)\\in[0,1]^2.\\) The assertion then follows from the conditional version of the Fundamental Theorem of Simulation. Left-continuity is immediate. For monotonicity, write \\[ p_\\tau(x_2,x_3) := v_{00}(\\tau) + (v_{10}(\\tau) - v_{00}(\\tau))x_2 + (v_{01}(\\tau)-v_{00}(\\tau))x_3 \\] for \\(\\tau\\in(0,1)\\) and \\((x_2,x_3)\\in[0,1]^2.\\) If \\(\\varsigma\\ge\\tau,\\) then the assumption of monotonicity (item 2) tells us that, in each one of the four vertices \\((0,0),\\) \\((1,0),\\) \\((0,1)\\) and \\((1,1),\\) the affine hyperplane \\(p_\\varsigma\\) lies above the affine hyperplane \\(p_\\tau.\\) It is clear that this implies \\(p_\\varsigma(x_2,x_3)\\ge p_\\tau(x_2,x_3)\\) for any \\((x_2,x_3)\\) in the unit square, completing the proof. Remark. To generalize Proposition 4.1 to dimensions \\({\\mathrm{D}_X}&gt;3\\) one needs to specify functions \\(v_d\\colon(0,1)\\to\\mathbb{R},\\) \\(d\\in\\{0,1\\}^{{\\mathrm{D}_X}-1}\\) that are non-decreasing and left-continuous. These correspond to the values of the associated affine hyperplanes at the vertices of the \\({\\mathrm{D}_X}-1\\) dimensional hypercube \\([0,1]^{{\\mathrm{D}_X}-1}\\): \\(v_{0\\cdots0}\\) is the “intercept” (the height of the hyperplane at the origin), \\(v_{10\\cdots0}\\) is the height of the hyperplane at vertex \\((1 \\quad 0 \\cdots 0)^\\prime\\in\\mathbb{R}^{{\\mathrm{D}_X}-1}\\) and so on. Nonetheless, the number of “compatibility conditions” grows much faster than the dimension. For instance, with \\({\\mathrm{D}_X}=4\\) the functions \\(v_d\\colon(0,1)\\to\\mathbb{R},\\) \\(d\\in\\{0,1\\}^{{\\mathrm{D}_X}-1}\\) must be chosen in such a way that all of the mappings below are non-decreasing in \\(\\tau\\) \\[\\begin{align} v_{100}+v_{010} - v_{000} &amp;\\qquad v_{100}+v_{001}-v_{000}\\\\ \\\\ v_{010}+v_{001} - v_{100} &amp;\\qquad v_{100} + v_{010} + v_{001} - 2v_{000} \\end{align}\\] A second important remark here is that Proposition 4.1 lays out sufficient conditions ensuring that a given candidate function is a bona fide conditional quantile function: if one can find the “vertex functions” \\(v_d\\) having the required properties stated in the proposition, then one can assure that a conditional quantile function is at hand. It turns out the condition is also necessary, provided the support of the covariates is the whole set \\(\\{1\\}\\times[0,1]^{{\\mathrm{D}_X}-1},\\) and not only one of it’s subsets. The example below illustrates a case where the condition is not necessary. Example 4.9 Let \\(X = (1\\quad U\\quad V)^\\prime,\\) where \\((U,V)\\) is uniformly distributed in the triangle \\[ \\Delta = \\{(u,v)\\in\\mathbb{R}^2\\colon\\, 0\\le v\\le u\\le1\\}. \\] Assume \\(Y\\) is a scalar random variable such that \\[ Q_{Y|X}(\\tau|1,u,v) = \\tau -\\tau u - \\tau v,\\quad \\tau\\in(0,1),\\,(u,v)\\in\\Delta. \\] Extrapolating the above quantile function to the vertex \\((1,1,1)\\) yields a function which decreases in \\(\\tau,\\) but this is not an issue because \\((1,1)\\) is not a “possible value” assumed by \\((U,V).\\) Although it is not possible, in general, to ensure that \\(\\mathscr{X} = \\{1\\}\\times [0,1]^{{\\mathrm{D}_X}-1}\\); take, for example, a covariate vector \\(X = (1\\quad X_2\\quad X_3)^\\prime\\) with \\((X_2\\quad X_3)^\\prime\\) uniformly distributed in the disc with center \\(c = (^1/_2\\,,\\, ^1/_2)\\) and radius \\(r=1/2.\\)↩︎ That is, the constants \\({m}_d\\) and \\(\\bar{m}_d\\) are determined as the endpoints of the closed interval \\[ I_d = \\bigcap\\{I : I\\ \\textsf{is a closed interval and}\\ \\operatorname{support}(X_d)\\subseteq I\\} \\]↩︎ The word “mostly” in the previous assertion was employed informally, and the example is not 100% honest as we could have, say, \\(\\beta_2\\) as the quantile function of a translated asymmetric Beta distribution, in which case the covariate \\(X_2\\) would affect not only the location and scale of the response, but also its coefficient of asymmetry.↩︎ "],["quantile-regression-in-time-series.html", "5 Quantile regression in time series", " 5 Quantile regression in time series In the time series literature, the consecrated pedagogy recommends that dynamic models are introduced in terms of stochastic difference equations of the form \\[\\begin{equation} Y_t = h(\\textsf{past})+\\textsf{innovation}_t,\\qquad\\textsf{for all admissible}\\, t \\tag{5.1} \\end{equation}\\] for a suitable function \\(h,\\) usually accompanied by the assumption that innovations do not depend “too much” on the past, that they are in some sense “well behaved,” etc. This approach has the merit of conveying an intuitive, mechanistic content, as it (sort of) tells us how the random variable \\(Y_t\\) comes into being, given what has happened in the past.16 Nevertheless, a stochastic difference equation — just as ordinary differential equations, for that matter — may or may not admit a solution, which in turn may or may not be stationary, ergodic, etc. Existence of a solution, here, means one can find a probability law of a stochastic process that satisfies the required stochastic difference equations. Exercise 5.1 (AR(1) model) Let \\((U_t\\colon t\\in\\mathbb{Z})\\) be a doubly infinite sequence of iid Gaussian random variables, and let \\(\\alpha\\) be a real number with \\(|\\alpha|&lt;1.\\) Define \\[ Y_t := \\lim_{p\\to \\infty}\\sum_{\\ell=0}^{p} \\alpha^\\ell U_{t-\\ell},\\qquad t\\in\\mathbb{Z}. \\] Show that the above limit exists almost surely (use the Borel-Cantelli lemma) so the \\(Y_t\\)’s are well defined. Show that the sequence \\((Y_t\\colon\\,t\\in\\mathbb{Z})\\) satisfies the stochastic difference equations \\[\\begin{equation} Y_t = \\alpha Y_{t-1} + U_t,\\qquad t\\in\\mathbb{Z} \\end{equation}\\] Find the conditional quantile function of \\(Y_t\\) given \\(Y_{t-1}.\\) Is it the same as the conditional quantile function of \\(Y_t\\) given \\((Y_{t-1},\\dots,Y_{t-p})\\) for arbitrary \\(p\\ge1?\\) \\(\\blacksquare\\) The idea of presenting time series models in terms of stochastic difference equations with “innovation terms” is reminiscent of the ubiquity of “error terms” in regression models. Well, while “observable” random variables work as a formal model for observable, quantitative phenomena, one may argue that error terms are a fiction (as they are commonly preceded by the adjective “unobservable”), and writing them down in an equation certainly does not concede them any material substance.17 An alternative route is to present regression models in terms of the conditional distribution of the response given the covariates; as we have seen in the preceding sections, this is precisely how quantile regression models proceed, and fortunately this can be brought to a time series framework. Before taking the first step in this direction, however, we need to come up with a precise definition of the idea of “conditioning on the entire past.” For that end, suppose that \\(\\big((Y_t,Z_t)\\colon\\,t\\in\\mathbb{Z}\\big)\\) is a stochastic process18 where, for all \\(t,\\) \\(Y_t\\) is scalar valued and \\(Z_t\\) is \\(\\mathbb{R}^{\\mathrm{D}_Z}\\) valued. For each \\(t\\in \\mathbb{Z},\\) let \\(\\mathfrak{F}_t\\) denote the \\(\\sigma\\)-field generated by the sequence of random vectors \\(\\big((Y_s,Z_s)\\colon s\\le t\\big).\\) For short, we shall write \\[ \\mathbf{P}[E\\,|\\,\\mathfrak{F}_{t-1}] := \\mathbf{P}[E\\,|\\,(Y_{s},Z_{s}),\\,s &lt; t],\\quad t\\in \\mathbb{Z} \\] for each event \\(E\\) determined by the \\(Y\\)’s and \\(Z\\)’s. Of special interest here are events \\(E\\) of the form \\(E = [Y_t\\le y, Z_t\\le z]\\) with \\(y\\in\\mathbb{R}\\) and \\(z\\in \\mathbb{R}^{\\mathrm{D}_Z}\\) (vector inequalities are interpreted component-wise), as the Kolmogorov Extension Theorem tells us that the probability law of the process \\(\\big((Y_t,Z_t)\\colon\\,t\\in\\mathbb{Z}\\big)\\) can be reconstructed from the sequence of conditional distributions \\[ \\mathbf{P}[Y_t\\le\\cdot, Z_t\\le \\cdot\\,|\\,\\mathfrak{F}_{t-1}],\\quad t\\in\\mathbb{Z}. \\] The notation introduced above calls for a bit of caution, as the conditional probabilities \\(\\mathbf{P}[E\\,|\\,\\mathfrak{F}_t]\\) are random variables: they are in analogy with our previous \\(\\mathbf{P}[E\\,|\\,X],\\) and not with the more down-to-earth \\(\\mathbf{P}[E\\,|\\, X=x].\\) This has the drawback of requiring us to introduce additional notation for the conditional quantile functions. We shall write, for \\(\\tau\\in(0,1)\\) \\[\\begin{equation} Q_{Y_t}(\\tau\\,|\\,\\mathfrak{F}_{s}) := \\inf\\{y\\in\\mathbb{R}\\colon\\, \\mathbf{P}[Y_t\\le y\\,|\\,\\mathfrak{F}_{s}] \\ge \\tau\\},\\quad t,s\\in \\mathbb{Z}. \\end{equation}\\] The careful reader will notice the random set in the right-hand side above, which may raise concerns about measurability. The thing is, notational extravagances aside, what really is going on here is that, at the outset, we are working with a sequence of regular conditional distributions of the form \\[ \\pi_t(E,v) = \\mathbf{P}[E\\,|\\,V_t = v]\\quad t\\in\\mathbb{Z}, \\] where \\(V_t\\) is the stochastic process \\(\\big((Y_s,Z_s)\\colon s\\le t\\big)\\) and where \\(v\\) runs through admissible values of \\(V_t\\) — that is, \\(v\\in\\operatorname{support}(V_t).\\) In this case we have \\(\\mathbf{P}[E\\,|\\,\\mathfrak{F}_{t}] = \\pi_t(E, V_t)\\) and then \\(Q_{Y_t}(\\tau\\,|\\,\\mathfrak{F}_{t-1})\\) is simply the composition of the function \\(v\\mapsto Q_{Y_t|V_{t-1}}(\\tau\\,|\\,v)\\) with the “infinite dimensional random vector” \\(V_{t-1}.\\) That is, we have \\[ Q_{Y_t}(\\tau\\,|\\,\\mathfrak{F}_{t-1})(\\omega) = Q_{Y_t|V_{t-1}}(\\tau\\,|\\,V_{t-1}(\\omega)) \\] and so on. I hope this illustrates how clumsy notation can get, and here is one of those places I mentioned in the foreword, where ambiguity gets in the way. Exercise 5.2 (Vector autoregression) Let \\((W_t\\colon\\,t\\in\\mathbb{Z})\\) be a stochastic process with state space \\(\\mathbb{R}^{{\\mathrm{D}_Z}+1}.\\) For each \\(t,\\) write \\(W_t = (Y_t\\quad Z_t^\\prime)^\\prime.\\) Assume that, for each \\(t,\\) conditional on \\(\\mathfrak{F}_{t-1},\\) the random vector \\(W_t\\) has a multivariate Gaussian distribution with mean \\(\\mathbf{A}W_{t-1},\\) for some (non-random) \\(({\\mathrm{D}_Z}+1)\\times({\\mathrm{D}_Z}+1)\\) matrix \\(\\mathbf{A},\\) and covariance matrix \\(\\boldsymbol{\\Sigma}_1.\\) The process \\((W_t\\colon\\,t\\in\\mathbb{Z})\\) is called a vector autoregressive process of order 1. For each \\(t,\\) find the conditional quantile function \\(\\tau \\mapsto Q_{Y_t}(\\tau\\,|\\,\\mathfrak{F}_{t-1}).\\) Most of us, I believe, think about probability models in terms of an iterative mechanism, akin to simulation, and this is what an equation like (5.1) encodes.↩︎ Actually, it is just a matter of writing e.g. \\(Y = X^\\prime\\beta + (Y - X^\\prime\\beta)\\) so it really all boils down to distributional assumptions about the random variable \\(Y - X^\\prime\\beta.\\)↩︎ Considering \\(\\mathbb{Z}\\) as the index set is a matter of convenience; the times could run through non-negative integers as well, requiring some minor adjustments in notation.↩︎ "],["linear-dynamic-quantile-regression-models.html", "5.1 Linear dynamic quantile regression models", " 5.1 Linear dynamic quantile regression models We are now in a position to introduce a quite general linear dynamic quantile regression model. Assume \\(\\big((Y_t,Z_t)\\colon\\,t\\in\\mathbb{Z}\\big)\\) is a stochastic process where, for each \\(t,\\) \\(Y_t\\) is a scalar random variable and \\(Z_t\\) is a \\({\\mathrm{D}_Z}\\)-dimensional random vector, and suppose that, for \\(\\tau\\in(0,1)\\) and \\(t\\in\\mathbb{Z},\\) the following time homogeneous quantile regression equation is satisfied, \\[\\begin{equation} Q_{Y_{t}}(\\tau|\\mathfrak{F}_{t-1}) = \\alpha_0(\\tau) + \\sum_{j=1}^{\\mathrm{lag}_Y}\\alpha_{j}(\\tau) g_j(Y_{t-j}) + \\sum_{\\ell=1}^{\\mathrm{lag}_Z}Z_{t-\\ell}^\\prime\\theta_\\ell(\\tau) \\tag{5.2} \\end{equation}\\] for some (unknown) functions \\(\\alpha_j\\colon(0,1)\\to\\mathbb{R},\\) \\(j\\in\\{0,\\dots,{\\mathrm{lag}_Y}\\}\\) and \\(\\theta_\\ell\\colon(0,1)\\to\\mathbb{R}^{\\mathrm{D}_Z},\\) \\(\\ell\\in\\{1,\\dots,{\\mathrm{lag}_Z}\\},\\) and some (typically known) functions \\(g_j,\\) \\(j\\in\\{1,\\dots,{\\mathrm{lag}_Y}\\}.\\) In view of the discussion in the preceding sections, we can assume for convenience that \\(\\operatorname{support}(Z_t)\\subseteq[0,1]^{\\mathrm{D}_Z}\\) and that the functions \\(g_j\\) map the real line into the unit interval (so \\(\\operatorname{support}(g_j(V))\\subseteq[0,1]\\) for any random variable \\(V\\) and all \\(j\\)), in order to ensure the non-negativity and boundedness which in turn allow for greater flexibility on the functional forms of the \\(\\alpha\\)’s and \\(\\theta\\)’s in (5.2).19 Exercise 5.3 Show that, for any random variable \\(Y\\), if \\(G\\colon\\mathbb{R}\\to\\mathbb{R}\\) is an increasing function, then \\(Q_Y = G^{-1}\\circ Q_{G(Y)}.\\) Conclude that substituting \\(Q_{Y_t}(\\tau|\\mathfrak{F}_{t-1})\\) by \\(Q_{G(Y_t)}(\\tau|\\mathfrak{F}_{t-1})\\) in (5.2), with \\(G\\) as above, does not really add generality to the model. \\(\\blacksquare\\) Now, if for a fixed \\(t\\) we write \\(Y := Y_t,\\) \\[ X := \\begin{pmatrix} 1 &amp; g_1(Y_{t-1}) &amp; \\cdots &amp; g_{\\mathrm{lag}_Y}(Y_{t-{\\mathrm{lag}_Y}})&amp;Z_{t-1}^\\prime &amp; \\cdots &amp; Z_{t-{\\mathrm{lag}_Z}}^\\prime\\end{pmatrix}^\\prime \\] and \\(\\beta := (\\alpha_0\\quad \\alpha_1\\,\\cdots\\, \\alpha_{\\mathrm{lag}_Y}\\quad \\theta_1^\\prime\\,\\cdots\\,\\theta_{\\mathrm{lag}_Z}^\\prime),\\) then — voilà — we are back to the (hopefully familiar by now) standard, static linear quantile regression model of the previous sections. Indeed, with this notation we have \\[ Q_{Y_t}(\\tau|\\mathfrak{F}_{t-1}) = Q_{Y|X}(\\tau|X) = X^\\prime\\beta(\\tau),\\quad\\tau\\in(0,1). \\] Exercise 5.4 Let \\((W_t)_{t\\ge0}\\) be a Markov chain with state space \\(S=\\{0,1\\}\\), initial distribution \\(\\lambda_W\\) and transition matrix \\[ P = \\begin{pmatrix} p_{00} &amp; p_{01}\\\\ p_{10} &amp; p_{11} \\end{pmatrix}. \\] Also, let \\((Y_t)_{t\\ge0}\\) be a stochastic process with state space \\(S\\), initial distribution \\(\\lambda_Y\\) and assume that, for \\(\\tau\\in(0,1)\\) and \\(t\\ge1\\), the following holds, \\[ Q_{Y_{t}}(\\tau\\,|\\,\\mathfrak{F}_{t-1}) = \\alpha_0(\\tau) + \\alpha_1(\\tau)Y_{t-1} + \\theta_1(\\tau)W_{t-1} + \\theta_2(\\tau)W_{t-1}Y_{t-1} \\] where \\(\\mathfrak{F}_{t} = \\sigma\\big((Y_s,W_s)\\colon\\,s\\le t\\big)\\) and where, for \\(\\tau\\in(0,1)\\), the coefficients \\(\\alpha_1, \\alpha_2, \\theta_1\\) and \\(\\theta_2\\) are defined by \\[\\begin{align*} \\alpha_0(\\tau) &amp;= \\mathbb{I}_{[1/4\\le \\tau&lt;1]}\\\\ \\alpha_1(\\tau) &amp;= \\mathbb{I}_{[1/4\\le\\tau&lt;1/2]}\\\\ \\theta_1(\\tau) &amp;= \\mathbb{I}_{[1/4\\le\\tau&lt;1/2]}\\\\ \\theta_2(\\tau) &amp;= \\mathbb{I}_{[1/4\\le\\tau&lt;1/2]} - \\mathbb{I}_{[1/2\\le\\tau&lt;3/4]}. \\end{align*}\\] Show that, conditional on \\(\\mathfrak{F}_{t}\\), the random variable \\(Y_{t+1}\\) follows a \\(\\mathsf{Bernoulli}(V_t)\\) distribution, where \\[\\begin{equation*} V_t = \\begin{cases} 1/4 &amp; \\textsf{if $W_t=0$ and $Y_t=0$}\\\\ 1/2 &amp; \\textsf{if ($W_t=1$ and $Y_t=0$) or ($W_t=0$ and $Y_t=1$)}\\\\ 3/4 &amp; \\textsf{if $W_n=1$ and $Y_n=1$} \\end{cases} \\end{equation*}\\] Show that \\((Y_t,W_t)_{t\\ge0}\\) is a Markov chain with state space \\(S\\times S\\) and find the respective transition matrix. This exercise provides a very simple example of a stochastic process \\(\\big((Y_t,Z_t)\\colon t\\ge0\\big),\\) where \\(Z_t=(W_t\\quad W_tY_t)^\\prime,\\) satisfying the time homogeneous quantile regression equation (5.2). \\(\\blacksquare\\) When the functions \\(g_1,\\dots,g_{\\mathrm{lag}_Y}\\) are all equal to the identity function, the model described in equation (5.2) is precisely the QADL20 model of Galvao, Montes-Rojas, and Park (2013), up to the definition of \\(\\mathfrak{F}_t\\) and inclusion of a contemporaneous \\(Z_t\\) (notice, however, that the “minimum lag of \\(Z\\)” is arbitrary: we can always define \\(\\tilde{Z}_{t} = Z_{t-1}\\) in which case equation (5.2) includes a contemporaneous \\(\\tilde{Z}_t\\)). If additionally the \\(\\theta\\)’s are all equal to the zero function, then (5.2) corresponds to the QAR21 model of Koenker and Xiao (2006). Some remarks: first, with this notation the QADL model as introduced by Galvao, Montes-Rojas, and Park (2013) would have \\(\\sigma\\big\\{(Y_s,\\tilde{Z}_s)\\colon s\\le t\\big\\}\\) in place of \\(\\mathfrak{F}_{t-1},\\) which is likely a typo since \\(Y_t\\) behaves like a constant when conditioned on itself. Second, it is also important to keep in mind that equation (5.2) does not fully specify the dynamic behavior of the multivariate time series \\(\\big((Y_t,Z_t)\\colon\\,t\\in\\mathbb{Z}\\big).\\) Rather, it determines a class of models. This is a feature prevalent when dynamics are introduced via stochastic difference equations, for example those characterizing the class of \\(\\mathsf{VAR}(1)\\) models, in which (as mentioned earlier) a solution consists of a probability law of a stochastic process which obeys said equations. The solutions (which may or may not be stationary) will depend on the parameter matrix and on assumptions about the distribution of the innovation terms. Such a feature is particularly salient when it comes to the QADL equations, as there might be more than one probability law for \\(\\big((Y_t,Z_t)\\colon\\, t\\in\\mathbb{Z}\\big)\\) according to which (5.2) holds, even for the same set of functional parameters (the \\(\\alpha\\)‘s and \\(\\theta\\)’s). The piece that is missing here (in order to fully specify the dynamics of the process, up to the functional parameters) are the conditional distributions \\(\\mathbf{P}[Z_t\\in \\cdot\\,|\\,\\mathfrak{F}_{t-1}, Y_t],\\,t\\in\\mathbb{Z}\\). Third, one can find in the literature several time series models for bounded random variables. In many cases, these fall into a fully parametric framework (for example the \\(\\beta\\)ARMA model) — hence, if one is interested in estimating the conditional quantile functions, the “correct” procedure would be to estimate the model parameters via (quasi/pseudo)maximum likelihood,22 and then recover the desired conditional quantile functions by plugging in the estimated values into the (known, up to parameters) formula for the theoretical distribution. In general these parametric models will have a conditional quantile function which is non-linear in the parameters. Example 5.1 Assume that \\(Y_t|\\mathfrak{F}_{t-1}\\) has a Kumaraswamy distribution with parameters \\(Y_{t-1}/\\alpha\\) and \\(Y_{t-1}/\\beta\\), where \\(\\alpha,\\beta&gt;0\\). The quantile function of a random variable \\(V\\) having the Kumaraswamy distribution with parameters \\(a&gt;0\\) and \\(b&gt;0\\) is given by \\[ Q_V(\\tau)=(1-(1-\\tau)^\\frac{1}{b})^\\frac{1}{a},\\quad\\tau\\in(0,1), \\] so our assumption is that \\[ Q_{Y_t}(\\tau|\\mathfrak{F}_{t-1}) = (1-(1-\\tau)^\\frac{\\alpha}{Y_{t-1}})^\\frac{\\beta}{Y_{t-1}},\\quad\\tau\\in(0,1) \\] This model is not nested in (5.2). In order that everything is well posed, we also have to preclude linear dependence between the random variables appearing as predictors in the right-hand side of (5.2) (including the constant random variable). In fact, we need to require some additional technical measurability constraints as well.↩︎ For Quantile autoregressive distributed lag.↩︎ Quantile autoregressive, but you had already guessed this one.↩︎ As we will see later, estimation of the parameters in (5.2) is achieved by a different optimization problem.↩︎ "],["affine-hyperplanes-again.html", "5.2 Affine hyperplanes, again", " 5.2 Affine hyperplanes, again In a time series framework , as Koenker (2005) points out, quantile crossing is a more stringent restriction than it is in a cross-sectional set up. Indeed, as I have insisted so far, in a linear quantile regression model it is very convenient (and also not that restrictive) to impose boundedness and non-negativity on any variables appearing in the right-hand side of the defining equation, as a means to allow for greater flexibility in the functional form of the parameters. But then, under strict stationarity and presence of autoregressive terms these impositions necessarily apply to the response as well, since in this case \\(Y_t\\) and, say, \\(Y_{t-1}\\) are equal in distribution. To sum up, if we are interested in studying the class of stationary stochastic processes \\(\\big((Y_t, Z_t)\\colon t\\in\\mathbb{Z}\\big)\\) having a probability law for which the time homogeneous quantile regression equation \\[\\begin{equation} Q_{Y_{t}}(\\tau|\\mathfrak{F}_{t-1}) = \\alpha_0(\\tau) + \\sum_{j=1}^{\\mathrm{lag}_Y}\\alpha_{j}(\\tau) Y_{t-j} + \\sum_{\\ell=1}^{\\mathrm{lag}_Z}Z_{t-\\ell}^\\prime\\theta_\\ell(\\tau) \\tag{5.3} \\end{equation}\\] holds for \\(\\tau\\in(0,1)\\) and \\(t\\in\\mathbb{Z}\\), then necessarily we have to restrict our attention to dynamics having as state space a bounded subset of \\([0,+\\infty)^{1+{\\mathrm{D}_Z}}\\). As a matter of fact, we lose no generality in assuming that \\(\\operatorname{support}(Y_t,Z_t)\\subseteq [0,1]^{1+{\\mathrm{D}_Z}}\\). Therefore, we can resort to (a slightly modified version of) Proposition 4.1, and its multidimensional generalization, as a shortcut to specify the conditional quantile function of \\(Y_t\\) given \\(\\mathfrak{F}_{t-1}\\) via the underlying “vertex functions.” Example 5.2 Let \\(v_{00}, v_{10}\\) and \\(v_{01}\\) be the quantile functions corresponding to the Beta distribution with parameters, respectively, \\((a_{00},b_{00}),\\) \\((a_{10},b_{10})\\) and \\((a_{01},b_{01})\\), where \\(a_{ij}\\) and \\(b_{ij}\\) are positive real constants. Assume further that the function \\(v_{11} := v_{10} + v_{01} - v_{00}\\) is nondecrasing (notice that this will ensure that the range of \\(v_{11}\\) is contained in the unit interval, as \\(v_{11}(0)=0\\) and \\(v_{11}(1)=1.\\)) Now let \\(\\alpha_{0} = v_{00}\\), \\(\\alpha_{1} = v_{10} - v_{00}\\) and \\(\\theta_{1} = v_{01} - v_{00}\\), take scalar random variables \\(Y_0\\) and \\(Z_0\\) supported on the unit interval, and let \\(U_1, U_2, \\dots\\) and \\(V_1, V_2, \\dots\\) be iid sequences of Uniform\\([0,1]\\) random variables (with both sequences mutually independent and also independent from \\(Y_0\\) and \\(Z_0\\)). Define now, for \\(t\\ge1\\), recursively, \\[\\begin{align} Y_{t} &amp;= \\alpha_0(U_t) + \\alpha_1(U_t)Y_{t-1} + \\theta_1(U_t)Z_{t-1}\\\\[2pt] Z_{t} &amp;= Q_t(V_t, Z_{t-1},\\dots, Z_{0}, Y_{t}, Y_{t-1},\\dots, Y_0) \\end{align}\\] where, for all \\(t\\ge1\\), the function \\(Q_t:(0,1)\\times[0,1]^{2t+1}\\to[0,1]\\) is a quantile function on its first argument, whose range is contained in the unit interval. Then it holds that, for all \\(\\tau\\in(0,1)\\) and all \\(t\\ge1\\), \\[\\begin{equation} Q_{Y_t}(\\tau|\\mathfrak{F}_{t-1}) = \\alpha_0(\\tau) + \\alpha_1(\\tau)Y_{t-1} + \\theta_1(\\tau)Z_{t-1} \\end{equation}\\] Notice that the process \\(\\big((Y_t,Z_t)\\colon t\\ge0\\big)\\) may or may not be stationary, Markovian, ergodic, etc. These properties will hold or not depending on the initial distribution of the pair \\((Y_0,Z_0)\\) and on the “transition quantile functions” \\(Q_t\\); in fact, the equation for \\(Y_t\\) strongly suggests the Markov property, but \\(Z_t\\) may display longer dependence. The Markov property is attained whenever one can find a function \\(Q^*\\colon(0,1)\\times[0,1]^3\\) such that \\(Q_t(\\tau,z_{t-1},\\dots,z_0, y_{t}, y_{t-1}, \\dots, y_0) = Q^*(\\tau,z_{t-1},y_{t},y_{t-1})\\) for every \\(t\\ge1\\) and every \\(y_0,\\dots,y_t,z_0,\\dots,z_{t-1}\\in[0,1].\\) Below is a code which simulates from the above model. # Parameters for the Beta quantile functions v10, v01 and v00 {a10 = 1; b10 = 3; a01 = 5; b01 = 1; a00 = 3; b00 = 1} # Conditional quantile function of Y[t] given Y[t-1] = 1 and X[t-1]=0 v10 = function (tau) qbeta(tau, a10,b10) # Conditional quantile function of Y[t] given Y[t-1] = 0 and X[t-1]=1 v01 = function(tau) qbeta(tau,a01,b01) # Conditional quantile function of Y[t] given Y[t-1] = 0 and X[t-1]=0 v00 = function(tau) qbeta(tau,a00,b00) # Conditional quantile function of Y[t] given Y[t-1] = 1 and X[t-1]=1 v11 = function(tau) v10(tau) + v01(tau) - v00(tau) # Functional parameters for the quantile regression equation alpha0 = v00 alpha1 = function(tau) v10(tau) - v00(tau) theta1 = function(tau) v01(tau) - v00(tau) # Quantile function of Y given F[t-1] Q = function(tau,Y.current,Z.current){ alpha0(tau) + alpha1(tau)*Y.current + theta1(tau)*Z.current } # Simulating the sample paths: # Arbitrary starting point (Y0,Z0) Y0 = runif(1) Z0 = runif(1) Y = Z = numeric() Z.current = Z0 Y.current = Y0 T = 10001 for (t in 1:T){ # Simulates Y[t] given F[t-1] using the Fundamental Theorem of Simulation Y[t] = Q(runif(1), Y.current, Z.current) Z[t] = Q(runif(1), Z.current, Y.current) #(obs: here Z follows the &quot;same&quot; dynamics as Y, but this can be changed. For instance, we could have Z[t]=runif(1), etc) Z.current = Z[t] Y.current = Y[t] } # ACF plot of acf(Y, lwd=16, lend=3, col=&#39;gray&#39;) # Scatterplot of Y aggainst lagged values of Y plot(Y[2:T]~Y[1:(T-1)], pch=16, col=rgb(0,0,0,.4)) # Scatterplot of Y aggainst lagged values of Z plot(Y[2:T]~Z[1:(T-1)], pch=16, col=rgb(0,0,0,.4)) # Histogram of Y[1],..., Y[T] hist(Y, border=NA, breaks=&quot;FD&quot;) "],["references.html", "6 References", " 6 References Aliprantis, Charalambos D., and Kim C. Border. 2006. Infinite Dimensional Analysis: A Hitchhiker’s Guide. 3rd ed. Springer. Billingsley, Patrick. 1995. Probability and Measure. 3rd ed. John Wiley &amp; Sons. Galvao, Antonio F., Gabriel Montes-Rojas, and Sung Y. Park. 2013. “Quantile Autoregressive Distributed Lag Model with an Application to House Price Returns.” Oxford Bulletin of Economics and Statistics 75 (2): 307–21. https://doi.org/10.1111/j.1468-0084.2011.00683.x. Gowrisankaran, Kohur. 1972. “Measurability of Functions in Product Spaces.” Proceedings of the American Mathematical Society 31 (2): 485–88. Koenker, Roger. 2005. Quantile Regression. Cambridge University Press. Koenker, Roger, and Gilbert Bassett. 1978. “Regression Quantiles.” Econometrica 46 (1): 33–50. Koenker, Roger, Victor Chernozhukov, X. He, and L. Peng, eds. 2017. CRC Press. Koenker, Roger, and Zhijie Xiao. 2006. “Quantile autoregression.” Journal of the American Statistical Association 101 (475): 980–90. https://doi.org/10.1198/016214506000000672. van der Vaart, Aad W. 1998. Asymptotic Statistics. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press. Zheng, Qi, Limin Peng, and Xuming He. 2015. “Globally adaptive quantile regression with ultra-high dimensional data.” The Annals of Statistics 43 (5): 2225–58. https://doi.org/10.1214/15-AOS1340. "]]
