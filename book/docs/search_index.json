[["index.html", "Quantile Regression: Facts and digressions 1 Exordium", " Quantile Regression: Facts and digressions Eduardo Horta Department of Statistics — UFRGSeduardo.horta@ufrgs.br 2021-04-09 1 Exordium These notes are intended as a place where I’ll gather interesting facts about quantiles and quantile regression (from elementary to fringe). This is definitely not a textbook: for that you can consult Koenker (2005) and Koenker et al. (2017). In any case I have included many exercises that clarify curious and relevant facts that appear scattered in the literature. It is assumed that the reader has some knowledge of Probability and Statistics. The expression “measurable function” appears many times in the text; if you don’t know what it means, substitute “measurable” by “piecewise continuous.” You won’t lose much. In the Probability and Statistics literature, it has become a habit to adopt the notational abuse of writing \\(g(X)\\) when one really means the composition \\(g \\circ X.\\) There is no easy way around, and sometimes ambiguity does arise.1 At any rate, in most cases one can tell from context what is the correct interpretation, that is, in writing \\(g(X)\\) we usually know beforehand what the symbol \\(g\\) stands for. For example, if \\(X\\) is a random variable and \\(g\\) is a real valued measurable function on \\(\\mathbb{R}\\), then \\(g(X)\\) stands for \\(g\\circ X\\). Similarly, if \\(g\\) is a functional or operator, say \\(g = \\mathbf{E}\\), then \\(\\mathbf{E}(X)\\) is not a composition but evaluation of the functional \\(\\mathbf{E}\\) at the “point” \\(X\\). In view of this, I shall (in dismay) go along with tradition. For instance, for a random vector \\(X\\) the notation \\(\\Vert X\\Vert_2\\) can be used both for the “random Euclidean norm” \\(\\Vert X\\Vert_2 = \\sqrt{\\sum\\nolimits_{d=1}^D X_d^2}\\) and for the proper \\(L^2\\) norm, \\(\\Vert X\\Vert_2 = \\mathbf{E}\\sqrt{\\sum\\nolimits_{d=1}^D X_d^2}.\\)↩︎ "],["intro.html", "2 Intro: distributions and quantiles", " 2 Intro: distributions and quantiles In Science, probability distributions are one of the main ways through which uncertainty about phenomena is modeled and quantified. In the most basic setting, we have a random variable, say \\(Y\\), that represents the numerical outcome of an experiment. Formally, \\(Y\\) is modeled as a measurable function defined on some (abstract/mathematical) measurable space \\((\\Omega,\\mathscr{F})\\). Each probability measure \\(\\mathbf{P}\\) on \\((\\Omega,\\mathscr{F})\\) then induces a probability measure on \\(\\mathbb{R}\\), called the distribution of \\(Y\\) and denoted by \\(\\mathbf{P}_Y\\), defined through \\[\\begin{equation} \\mathbf{P}_Y(B) := \\mathbf{P}[Y\\in B] \\tag{2.1} \\end{equation}\\] for each Borel subset \\(B\\subseteq\\mathbb{R}\\). Importantly, as a consequence of Carathéodory’s Extension Theorem, the measure \\(\\mathbf{P}_Y\\) is recoverable from a much simpler function, namely the cumulative distribution function of \\(Y\\), denoted by \\(F_Y\\) (of course, \\(F_Y\\) depends implicitly on \\(\\mathbf{P}\\)) and defined by \\[\\begin{equation} F_Y(y) := \\mathbf{P}_Y(-\\infty,y] = \\mathbf{P}[Y\\le y],\\qquad y\\in\\mathbb{R}. \\end{equation}\\] The preceding assertion means that the task of cooking up a probability distribution for a scalar random variable \\(Y\\) boils down to exhibiting its cumulative distribution function. This is nice because probability measures are not computationally tractable, whereas cumulative distribution functions, being representable through algebraic expressions or at least via numerical formulas, are. To sum up, the message is that if we want to build a model to quantify uncertainty about a scalar phenomenon, all we have to do is to propose a non-decrasing, right-continuous function \\(F\\colon\\mathbb{R}\\to\\mathbb{R}\\) that satisfies the requirements \\(\\lim_{y\\to -\\infty}F(y) = 0\\) and \\(\\lim_{y\\to+\\infty}F(y) = 1\\). Such functions exhaust, in a one-to-one fashion, the category of univariate probability measures. Example 2.1 Let \\(f\\colon \\mathbb{R}\\to \\mathbb{R}\\) be defined through \\[\\begin{equation} f(y) := \\frac{1}{\\sqrt{2\\pi}}\\mathrm{e}^{-y^2/2},\\qquad y\\in\\mathbb{R}. \\end{equation}\\] Put \\(\\Omega = \\mathbb{R}\\) and let \\(\\mathscr{F}\\) be the class of Borel subsets of \\(\\Omega\\). Also, define \\[\\begin{equation} F(y) := \\int_{-\\infty}^{y} f(u)\\,\\mathrm{d}u,\\qquad y\\in \\mathbb{R}. \\end{equation}\\] If we now let \\(\\mathbf{P}\\) be the unique probability measure (given by Carathéodory’s Theorem) on \\((\\Omega,\\mathscr{F})\\) satisfying the equality \\(\\mathbf{P}(-\\infty,y] = F(y)\\) for all \\(y\\in\\mathbb{R}\\) , then \\(\\mathbf{P}\\) is called the standard Gaussian distribution on \\(\\mathbb{R}\\). Additionally, defining the random variable \\(Y\\) through \\[\\begin{equation} Y(\\omega) := \\omega,\\qquad \\omega\\in\\mathbb{R}, \\end{equation}\\] it is clear that \\(Y\\) has distribution function \\(\\mathbf{P}\\) and cumulative distribution function \\(F\\), that is, \\(\\mathbf{P}_Y = \\mathbf{P}\\) and \\(F_Y = F\\). Such \\(Y\\) is called a standard Gaussian (or: standard Normal) random variable. "],["quantiles-and-ranks.html", "2.1 Quantiles and ranks", " 2.1 Quantiles and ranks We now show that there is an alternative approach to specify a probability measure on \\(\\mathbb{R}\\). Let \\(Y\\) be a random variable with cumulative distribution function \\(F_Y\\). For \\(\\tau\\in(0,1),\\) the \\(\\tau\\)-th quantile of \\(Y\\) is the real number \\(Q_Y(\\tau)\\) defined via \\[\\begin{equation} Q_Y(\\tau) := \\inf\\{y\\in\\mathbb{R}\\colon\\, F_Y(y)\\ge\\tau\\}. \\end{equation}\\] The function \\(\\tau \\mapsto Q_Y(\\tau)\\) from \\((0,1)\\) to \\(\\mathbb{R}\\) is called the quantile function of \\(Y\\) (also known as its generalized inverse of \\(F_Y\\)). Writing \\(F = F_Y\\) and \\(Q = Q_Y\\) for simplicity, it is an exercise (see van der Vaart (1998), Chapter 21) to check that \\(Q\\) is non-decreasing and left-continuous and that, for any \\(y\\in\\mathbb{R}\\) and \\(\\tau\\in(0,1)\\), one has \\[\\begin{equation} Q(\\tau)\\le y \\quad \\textsf{if and only if}\\quad \\tau\\le F(y). \\end{equation}\\] The latter equivalence has at least two important consequences: first, it shows that2 \\(F(y) = \\sup\\{\\tau\\in(0,1)\\colon\\,Q(\\tau)\\le y\\}\\) and so \\(F\\) and \\(Q\\) are entirely recoverable from one another: knowing \\(F\\) we know \\(Q\\) and vice-versa. Second, one sees that whenever \\(U\\) is a uniform random variable on the unit interval \\([0,1)\\), it holds that \\(Q(U)\\) has cumulative distribution function \\(F\\), that is, \\(Q(U)\\) and \\(Y\\) are equal in distribution. This result is sometimes called the Fundamental Theorem of Simulation.3 In the same spirit as in the preceding section, we can say that if our aim is to build a model to quantify uncertainty about a scalar phenomenon, all we have to do is to propose a non-decrasing, left-continuous function \\(Q\\colon(0,1)\\to\\mathbb{R}\\). Such functions exhaust, in a one-to-one fashion, the category of univariate probability measures. We have seen that univariate probability distributions are entirely characterized by their cumulative distribution functions, which in turn are entirely characterized by their corresponding quantile functions. A quantile function, notwithstanding, has the additional benefit of being not only a univariate probability model, but also a recipe to simulate from that model. Example 2.2 Fix a real number \\(\\lambda&gt;0\\), and let \\(Q\\colon(0,1)\\to\\mathbb{R}\\) be defined by \\[\\begin{equation} Q(\\tau) := -\\frac{\\log(1-\\tau)}\\lambda,\\qquad \\tau\\in(0,1). \\end{equation}\\] Notice that \\(Q\\) is a quantile function, being strictly increasing and continuous. Now let \\(\\Omega := [0,1)\\), put \\(\\mathscr{F} := \\hphantom{\\!}\\) “the collection of Borel subsets of \\(\\Omega\\),” and let \\(\\mathbf{P}\\) be the Lebesge measure on \\((\\Omega,\\mathscr{F})\\), that is, \\(\\mathbf{P}\\) is the unique Borel probability measure on \\([0,1)\\) for which the identity \\(\\mathbf{P}[a,b)=b-a\\) holds for all \\(0\\le a&lt;b\\le1\\). In this setup, define the random variable \\(Y\\) through \\[\\begin{equation} Y(\\omega) := Q(\\omega),\\qquad \\omega\\in\\Omega. \\end{equation}\\] What is the distribution of \\(Y\\)? We can compute this directly: for \\(y\\in\\mathbb{R}\\), we have \\[\\begin{align} \\mathbf{P}[Y &gt; y] &amp;= \\mathbf{P}\\{\\omega\\in[0,1)\\colon\\, -\\log(1-\\omega)&gt;\\lambda y\\}\\\\ &amp;= \\mathbf{P}\\{\\omega\\in[0,1)\\colon\\, \\omega &gt; 1 - \\mathrm{e}^{-\\lambda y}\\}. \\end{align}\\] Therefore, \\(\\mathbf{P}[Y&gt;y] = 1\\) for \\(y\\le0\\) and, for \\(y&gt;0\\), \\[\\begin{align} \\mathbf{P}[Y&gt;y] = \\mathbf{P}[1-\\mathrm{e}^{-\\lambda y}, 1) = 1 - (1-\\mathrm{e}^{-\\lambda y}) \\end{align}\\] by the definition of the Lebesgue measure \\(\\mathbf{P}\\). It follows that \\[ F_Y(y) = 1-\\mathrm{e}^{-\\lambda y},\\qquad y\\ge0 \\] and \\(F_Y(y) = 0\\) otherwise: \\(Y\\) is an Exponential random variable with parameter \\(\\lambda\\). Of course, a straightforward computation will tell us that \\(Q_Y = Q\\). Accordingly, if we define the random variable \\(U\\) to be the identity function on \\(\\Omega\\), then clearly \\(U\\) has a Uniform\\([0,1]\\) distribution, and in this particular example not only are \\(Q(U)\\) and \\(Y\\) equal in distribution as we stressed earlier, but actually \\(Q(U) = Y\\) pointwise. In any case, the fact that \\(Y = Q(U)\\) allows us to simulate “from the distribution \\(F_Y\\),” as long as we have available a mechanism to generate pseudo-random uniform random variables. The r code below illustrates the idea, sampling \\(n\\) independent Uniform\\([0,1]\\) pseudo-random numbers \\(u_1,\\dots,u_n\\) and displaying the histogram plot of \\(Q(u_1),\\dots,Q(u_n)\\). set.seed(1) n = 10000 lambda = 1 Usample = runif(n) Q = function(w) -log(1 - w)/lambda hist(Q(Usample), probability = TRUE, border = NA, breaks = &quot;Scott&quot;, xlab = &quot;y&quot;, main = NA) Exercise 2.1 Show that \\(\\mathbf{E}(Y) = \\int_0^1 Q_Y(\\tau)\\,\\mathrm{d}\\tau\\).4 Exercise 2.2 Show that: \\(\\mathbf{P}[Y\\le Q_Y(\\tau)]\\ge \\tau\\) and \\(\\mathbf{P}[Y\\ge Q_Y(\\tau)]\\ge 1-\\tau\\). \\(F_Y(Q_Y(\\tau)) = \\tau\\) if and only if \\(F_Y\\) is continuous at \\(Q_Y(\\tau)\\). Exercise 2.3 For \\(\\tau\\in(0,1)\\), let \\(\\rho_\\tau\\colon\\mathbb{R}\\to\\mathbb{R}\\) be defined through \\[\\begin{equation} \\rho_\\tau(u) = u(\\tau - \\mathbb{I}[u&lt;0]),\\quad u\\in\\mathbb{R}. \\end{equation}\\] Show that \\(2\\rho_\\tau(u) = (2\\tau-1)u + |u|\\). Show that \\(\\rho_\\tau(u)\\) is a convex function of \\(u\\). Show that \\(Y\\) is integrable if and only if \\(\\rho_\\tau(Y-y)\\) is integrable for all \\(y\\in\\mathbb{R}\\). Show that \\(\\rho_\\tau(Y - y) - \\rho_\\tau(Y)\\) is integrable for all \\(y\\in\\mathbb{R}\\).5 Show that, for all \\(y\\in\\mathbb{R}\\), one has \\[\\begin{equation} \\mathbf{E}\\rho_\\tau\\big(Y - Q_Y(\\tau)\\big) \\le \\mathbf{E}\\rho_\\tau(Y - y). \\end{equation}\\] Thus, the problem of minimizing the function \\(y\\mapsto \\mathbf{E}\\rho_\\tau(Y -y)\\) has at least one solution, and \\(Q_Y(\\tau)\\in\\arg\\min_y \\mathbf{E}\\rho_\\tau(Y-y)\\). Hint: consider first the case when \\(Y\\) is absolutely continuous. Show that, if \\(g\\colon(0,1)\\to\\mathbb{R}\\) is any measurable function, then \\[\\begin{equation} \\int_0^1\\mathbf{E}\\rho_\\tau\\big(Y - Q_Y(\\tau)\\big)\\,\\mathrm{d}\\tau \\le \\int_0^1\\mathbf{E}\\rho_\\tau\\big(Y - g(\\tau)\\big)\\,\\mathrm{d}\\tau. \\end{equation}\\] Exercise 2.4 Assume \\(Y\\) is \\(\\mathbf{P}\\)-absolutely continuous, with density function \\(f_Y\\). Use the Inverse Function Theorem to show that \\[ \\frac{\\mathrm{d}}{\\mathrm{d}\\tau}Q_Y(\\tau) = \\frac{1}{f_Y(Q_Y(\\tau))}, \\tau\\in(0,1) \\] (make aditional assumptions if necessary). Adopting the convention that \\(\\sup\\{\\tau\\in(0,1)\\colon\\,2&gt;3\\} = 0\\).↩︎ A partial reciprocal to this result says that whenever \\(F_Y\\) is continuous the random variable \\(F_Y\\circ Y\\) has a uniform distribution on the unit interval.↩︎ This identity is extremely useful as it allows us to bypass Lebesgue’s theory of integration in defining the integral \\(\\mathbf{E}(Y)\\): indeed, the function \\(\\tau\\mapsto Q_Y(\\tau)\\) is monotone, left continuous and has well defined right limits; thus, \\(Q_Y\\) is Riemann integrable on any compact interval \\([a,b]\\subseteq (0,1)\\). Now it is a matter of calling \\(Y\\) a \\(\\mathbf{P}\\)-integrable function iff \\(\\lim_{a\\downarrow0}\\lim_{b\\uparrow 1}\\int_a^b |Q_Y(\\tau)|\\,\\mathrm{d}\\tau &lt;\\infty\\) and then define \\(\\mathbf{E}(Y):=\\int_0^1 Q_Y(\\tau)\\,\\mathrm{d}\\tau.\\)↩︎ This item shows that the “correct” loss function to consider should be \\(L(y) = \\rho_\\tau(Y - y) - \\rho_\\tau(Y)\\) and not \\(L(y) = \\rho_\\tau(Y-y)\\) as one usually sees in the literature.↩︎ "],["conditioning-the-plot-thickens.html", "3 Conditioning: the plot thickens", " 3 Conditioning: the plot thickens We have seen that probability distributions (or cumulative distribution functions, or yet quantile functions) are a fundamental tool in modeling, describing and quantifying uncertainty about one-dimensional numerical phenomena. Unfortunately the univariate setting is too restrictive: a handful of applications in Statistics and Machine Learning (and many other Scientific fields) deal with a scenario where we have uncertainty “up to covariates,” meaning that the value of a given variable (possibly a vector) of interest (the response variable) is partially determined by the values of other variables (the covariates or regressors), the latter being known to the researcher (and maybe even under her control). Regression, in a myriad of guises, appears as one of the methods most widely employed to model the dependence structure between a response and covariates. Before moving on to regression proper, let us first introduce a precise notion of conditioning. In the statement below, \\(\\mathbb I\\) denotes the indicator function. Theorem 3.1 (Regular conditional distribution) Let \\(Y\\) be a real valued random variable and let \\(X\\) be a \\(D\\)-dimensional random vector. Then there exists a function \\((B,x)\\mapsto \\pi(B,x)\\) defined for Borel sets \\(B\\subseteq\\mathbb{R}\\) and \\(x\\in \\mathbb{R}^D\\), satisfying the following: for each fixed \\(x\\in\\mathbb{R}^D\\), the function \\(B\\mapsto \\pi(B,x)\\) is a Borel probability measure on \\(\\mathbb{R}\\). for each fixed Borel subset \\(B\\subseteq\\mathbb{R}\\), the function \\(x\\mapsto \\pi(B,x)\\) is measurable and integrable. It holds that \\[\\begin{equation} \\mathbf{P}[Y\\in B, X\\in A] = \\mathbf{E}\\big[\\pi(B,X)\\cdot\\mathbb{I}[X\\in A]\\big] \\tag{3.1} \\end{equation}\\] Moreover, the function \\(\\pi\\) above is essentially unique in the sense that, if \\(\\pi_0\\) is another function satisfying items 1 to 3, then there exists a Borel subset \\(A^*\\subseteq\\mathbb{R}^D\\) with \\(\\mathbf{P}[X\\in A^*]=1\\) such that \\(\\pi(B,x) = \\pi_0(B,x)\\) for all Borel subset \\(B\\subseteq\\mathbb{R}\\) and all \\(x\\in A^*\\). Remark. Theorem 3.1 still holds when \\(Y\\) is a random vector. We chose to state it in the simpler setting of a scalar \\(Y\\) since this is the case which will be more prevalent throughout the text. Also, the “essential uniqueness” of \\(\\pi\\) tells us in particular that the definition of \\(\\pi(\\cdot,x)\\) for \\(x\\) outside the support of \\(X\\) is arbitrary. The function \\(\\pi\\) appearing in Theorem 3.1 is called the regular conditional distribution of \\(Y\\) given \\(X\\). It is customary to write \\(\\pi(B,x) =: \\mathbf{P}[Y\\in B\\,|\\,X=x]\\) and call this right hand side the conditional probability of the event \\([Y\\in B]\\) given \\(X=x\\). It is also convenient to use the notation \\(\\pi(B,X) =: \\mathbf{P}[Y\\in B\\,|\\, X]\\) (this is a random variable) which is called the conditional probability of the event \\([Y\\in B]\\) given \\(X\\). The difference is subtle but relevant. Notice also that there is redundancy in writing \\(\\mathbf{P}[Y\\in B\\,|\\,X=x]\\): for if \\(\\mathbf{P}[X=x]&gt;0\\) for some \\(x\\), the elementary notion of conditional probability would entreat us to think of the equality \\[\\begin{equation} \\mathbf{P}[Y\\in B\\,|\\,X=x] = \\frac{\\mathbf{P}[Y\\in A, X=x]}{\\mathbf{P}[X=x]}, \\end{equation}\\] but \\(\\pi(B,x)\\) is not necessarily expressible as above. However, when \\(X\\) is discrete, this is precisely the case, as the following exemple illustrates. Example 3.1 Let \\(Y\\) be a random variable and \\(X\\) a \\(\\mathbf{P}\\)-discrete random vector of dimension \\(D\\). Then the function \\(\\pi\\) defined, for Borel subsets \\(B\\subseteq\\mathbb{R}\\) and \\(x\\in\\mathbb{R}^D\\), by the relation \\[\\begin{equation} \\pi(B,x)\\mathbf{P}[X=x] := \\mathbf{P}[Y\\in B, X=x] \\end{equation}\\] is a regular conditional distribution of \\(Y\\) given \\(X\\) (it is an exercise to check that this assertion is true!) Example 3.2 Let \\(Y\\) be a scalar random variable, and let \\(X\\) be a \\(D\\)-dimensional random vector. Assume that \\((Y,X)\\) is \\(\\mathbf{P}\\)-absolutely continuous, meaning that \\((Y,X)\\) have joint density function \\(f_{Y,X}\\).6 Then the function \\(\\pi\\) defined, for Borel subsets \\(B\\subseteq\\mathbb{R}\\) and \\(x\\in\\mathbb{R}^D\\), by \\[\\begin{equation} \\pi(B,x) := \\int_{B} f_{Y|X}(y|x)\\,\\mathrm{d}y, \\end{equation}\\] is a regular conditional distribution of \\(Y\\) given \\(X\\). In the above, \\(f_X\\) is the marginal density function of \\(X\\), i.e., \\(f_X(x) = \\int_\\mathbb{R}f_{Y,X}(y,x)\\,\\mathrm{d}y\\), and \\(f_{Y|X}\\) is the conditional density function of \\(Y\\) given \\(X\\), defined by the relation \\[\\begin{equation} f_{Y|X}(y|x)f_X(x) := f_{Y,X}(y,x) \\end{equation}\\] for \\(x\\in \\mathbb{R}^D\\) and \\(y\\in\\mathbb{R}\\). \\(\\blacksquare\\) It is important to notice that the equality in equation (3.1) can be rewritten as \\[\\begin{equation} \\mathbf{P}[Y\\in B, X\\in A] = \\int_A \\mathbf{P}(Y\\in B\\,|\\,X=x)\\,F_X(\\mathrm{d}x), \\tag{3.2} \\end{equation}\\] where “\\(\\int \\cdot \\,F_X(\\mathrm{d}x)\\)” is used to denote the Lebesgue-Stieltjes integral (\\(F_X\\) being the cumulative distribution function of \\(X\\)). Of course, whenever \\(X\\) is \\(\\mathbf{P}\\)-absolutely continuous, with density function \\(f_X\\), this integral reduces to \\[\\begin{equation} \\mathbf{P}[Y\\in B, X\\in A] = \\int_A \\mathbf{P}(Y\\in B\\,|\\,X=x)f_X(x)\\,\\mathrm{d}x. \\end{equation}\\] Similarly, if \\(X\\) is discrete with probability mass function \\(p_X\\), we have \\[\\begin{equation} \\mathbf{P}[Y\\in B, X\\in A] = \\sum_{\\{x\\in A\\,:\\,p_X(x)&gt;0\\}}\\mathbf{P}(Y\\in B\\,|\\,X=x) p_X(x) \\end{equation}\\] which, in view of example 3.1, is just the classical Law of Total Probability. The essentially unique non-negative function satisfying the identity \\(F_{Y,X}(y,x) = \\int_{-\\infty}^y\\int_{-\\infty}^x f_{Y,X}(u,v)\\,\\mathrm{d}v\\mathrm{d}u\\) for all \\(y\\in\\mathbb{R}\\) and \\(x\\in \\mathbb{R}^D\\). Here, \\(\\int_{-\\infty}^x \\mathrm{d}v\\) means \\(\\int_{-\\infty}^{x_1}\\cdots \\int_{-\\infty}^{x_D} \\mathrm{d}v_D\\cdots \\mathrm{d}v_1\\).↩︎ "],["the-substitution-principle.html", "3.1 The Substitution Principle", " 3.1 The Substitution Principle The random variable \\(Y\\) above is quite arbitrary: in particular, it may be of the form \\(Y = \\varphi(X,Z)\\), where \\(Z\\) is a random vector and where \\(\\varphi\\) is a given measurable function. If this is the case, one feels tempted to ask: “conditional on \\(X=x\\), does \\(X\\) behave as a constant?” The answer is affirmative: Theorem 3.2 Let \\(X\\) and \\(Z\\) be random vectors of dimensions \\(D\\) and \\(D^\\prime\\) respectively. If \\(\\varphi\\colon\\mathbb{R}^{D}\\times\\mathbb{R}^{D^\\prime}\\to\\mathbb{R}\\) is a measurable function, then there exists a Borel set \\(A^*\\subseteq\\mathbb{R}^D\\) such that \\[\\begin{equation} \\mathbf{P}[\\varphi(X,Z)\\in B\\,|\\, X=x] = \\mathbf{P}[\\varphi(x,Z)\\in B\\,|\\, X=x] \\end{equation}\\] for all Borel sets \\(B\\subseteq \\mathbb{R}\\) and all \\(x\\in A^*\\). Remark. The equality stated in the above theorem actually means that, for each Borel set \\(B\\subseteq \\mathbb{R}\\), one has \\[ \\int \\mathbf{P}[\\varphi(X,Z)\\in B\\,|\\,X=x]\\,F_X(\\mathrm{d}x) = \\int \\mathbf{P}[Z\\in B_x\\,|\\,X=x]\\,F_X(\\mathrm{d}x), \\] where, for each \\(x\\in\\mathbb{R}^D\\), \\(B_x := \\{z\\in \\mathbb{R}^{D^\\prime}\\colon\\,\\varphi(x,z)\\in B\\}.\\) Example 3.3 With the same notation as in the above theorem, assume the function \\(\\varphi\\) does not depend on its second argument, that is, for some \\(\\psi\\colon\\mathbb{R}^D\\to\\mathbb{R}\\) it holds that \\(\\varphi(x,z) = \\psi(x)\\) for all \\(x\\in\\mathbb{R}^D\\) and \\(z\\in\\mathbb{R}^{D^\\prime}\\). Then an easy check tells us that \\[\\begin{equation} \\mathbf{P}[\\psi(X)\\in B\\,|\\,X=x] = \\begin{cases} 0,&amp; \\textsf{if } \\psi(x)\\notin B\\\\ 1,&amp; \\textsf{if } \\psi(x)\\in B \\end{cases} \\end{equation}\\] for any Borel set \\(B\\subseteq\\mathbb{R}\\). That is, \\(\\mathbf{P}[\\psi(X)\\in B\\,|\\,X=x] = \\mathbb{I}[\\psi(x)\\in B]\\). In particular, \\(\\mathbf{P}[\\psi(X) = \\psi(x)\\,|\\,X=x] = 1\\) (not surprisingly). The above also is true with \\(\\psi = \\hphantom{\\!}\\) “the identity function” (valid when \\(D=1\\) or by generalizing Theorem 3.1 to vector-valued \\(Y\\).) \\(\\blacksquare\\) Exercise 3.1 Show that, if \\(Y\\) is independent from \\(X\\), then \\(\\mathbf{P}[Y\\in B\\,|\\,X=x] = \\mathbf{P}[Y\\in B]\\) for all admissible \\(B\\subseteq\\mathbb{R}\\) and all \\(x\\in\\mathbb{R}^D\\), that is, one can take \\(\\pi(B,x) = \\mathbf{P}[Y\\in B]\\) in Theorem 3.1. \\(\\blacksquare\\) "],["conditional-cdfs-and-quantile-functions.html", "3.2 Conditional CDFs and Quantile functions", " 3.2 Conditional CDFs and Quantile functions In section 2 we have put forth the case that univariate probability distributions are essentially the same thing as their corresponding cumulative distribution functions, which in turn are essentially the same thing as their quantile functions. Well, whenever \\(Y\\) is a scalar random variable (and \\(X\\) is a random vector), for a fixed \\(x\\in\\mathbb{R}^D\\) the real valued function \\[\\begin{equation} B\\mapsto \\mathbf{P}[Y\\in B\\,|\\, X=x] \\tag{3.3} \\end{equation}\\] is in fact a probability distribution on the Borel subsets of \\(\\mathbb{R}\\). Thus, it not only makes sense to define the corresponding cumulative distribution function, but it is also the case that the latter function fully characterizes the mapping in equation (3.3). The conditional cumulative distribution of \\(Y\\) given \\(X\\) is the function \\((y,x)\\mapsto F_{Y|X}(y|x)\\) defined on \\(\\mathbb{R}\\times\\mathbb{R}^D\\) by \\[\\begin{equation} F_{Y|X}(y|x) := \\mathbf{P}[Y\\le y\\,|\\,X=x]. \\end{equation}\\] Importantly, the machinery introduced so far allows us to write the joint cumulative distribution function of \\((Y,X)\\) as7 \\[\\begin{equation} F_{Y,X}(y,x) = \\int_{-\\infty}^x F_{Y|X}(y|u)\\,F_X(\\mathrm{d}u),\\quad y\\in\\mathbb{R}, x\\in\\mathbb{R}^D. \\tag{3.4} \\end{equation}\\] Exercise 3.2 Find an expression for \\(F_{Y|X}(y|x)\\): in terms of the joint density function \\(f_{Y,X}\\), assuming \\((Y,X)\\) is absolutely continuous. in terms of the joint mass function \\(p_{Y,X}\\), assuming \\((Y,X)\\) is discrete. \\(\\blacksquare\\) But this is a text about quantile regression, and now comes a fundamental definition in this direction: in the same setting as above, we let the conditional quantile function of \\(Y\\) given \\(X\\) be the function \\((\\tau,x)\\mapsto Q_{Y|X}(\\tau|x)\\) defined on \\((0,1)\\times\\mathbb{R}^D\\) by \\[\\begin{equation} Q_{Y|X}(\\tau|x) = \\inf\\{y\\in\\mathbb{R}\\colon\\, F_{Y|X}(y|x)\\ge\\tau\\}. \\end{equation}\\] Wrapping up what we have seen so far, the catch is that a probability distribution on \\(\\mathbb{R}\\times\\mathbb{R}^D\\) is entirely determined by the marginal distribution of \\(X\\) together with the conditional quantile function \\(Q_{Y|X}\\), since we can recover the conditional CDF \\(F_{Y|X}\\) from \\(Q_{Y|X}\\) and reconstruct the joint CDF \\(F_{Y,X}\\) via (3.4). From \\(F_{Y,X}\\) we can then compute the probability of any event of the form \\([Y\\in B, X\\in A ]\\) via the formula \\[ \\mathbf{P}[Y\\in B, X\\in A] = \\int_{B\\times A}\\,F_{X,Y}(\\mathrm{d}y, \\mathrm{d}x). \\] The idea is not a mere shenanigan: if \\(D=1\\) it provides us a recipe to simulate a pair \\((Y,X)\\) from the distribution \\(\\mathbf{P}_{Y,X}\\) as follows: generate two independent pseudo-random numbers \\(u_1\\) and \\(u_2\\) from the Uniform\\([0,1]\\) distribution set \\(x = Q_X(u_1)\\) and \\(y = Q_{Y|X}(u_2|x)\\). It follows that the pair \\((y,x)\\) is a pseudo-random draw from \\(\\mathbf{P}_{Y,X}\\). Theorem 3.3 Assume \\(Y\\) is a scalar random variable, and \\(X\\) is a \\(D\\)-dimensional random vector. Denote by \\(F_X\\) the cumulative distribution function of \\(X\\), and by \\(Q_{Y|X}\\) the conditional quantile function of \\(Y\\) given \\(X\\). Now let \\(U\\) be a Uniform\\([0,1]\\) random variable independent from \\(X\\), and define \\(\\tilde{Y} = Q_{Y|X}(U\\,|\\,X)\\). Then \\(\\tilde{Y} \\overset{\\textsf{dist}}= Y.\\) Exercise 3.3 Prove Theorem 3.3. Exercise 3.4 Let \\(Z := g(X) + h(X)Y\\) where \\(g\\) and \\(h\\) are real valued measurable functions on \\(\\mathbb{R}^D\\), \\(h\\) being non-negative. Show that \\(Q_{Z|X}(\\tau|x) = g(x) + h(x)Q_{Y|X}(\\tau|x)\\), for \\(\\tau\\in(0,1).\\) Exercise 3.5 For \\(\\tau\\in(0,1)\\), let \\(g_\\tau\\colon\\mathbb{R}^D\\to\\mathbb{R}\\) be a measurable function. Show that \\[\\begin{equation} \\mathbf{E}\\rho_\\tau\\big(Y - Q_{Y|X}(\\tau|x)\\big) \\le \\mathbf{E}\\rho_\\tau\\big(Y - g_\\tau(X)\\big). \\end{equation}\\] Show that, if the mapping \\((\\tau,x)\\mapsto g_\\tau(x)\\) is measurable, then \\[\\begin{equation} \\int_0^1\\mathbf{E}\\rho_\\tau\\big(Y - Q_{Y|X}(\\tau|x)\\big)\\,\\mathrm{d}\\tau \\le \\int_0^1\\mathbf{E}\\rho_\\tau\\big(Y - g_\\tau(X)\\big)\\,\\mathrm{d}\\tau. \\end{equation}\\] Of course, the integral can be simplified in the cases where \\(X\\) is discrete or continuous, as we discussed earlier.↩︎ "],["conditional-expectation.html", "3.3 Conditional Expectation", " 3.3 Conditional Expectation The standard approach in intermediary and advanced Probability textbooks is to introduce the conditional expectation of \\(Y\\) given \\(X\\) (this is defined for integrable \\(Y\\)) as any random variable \\(W\\) that satisfies the following two conditions: There exists a measurable function \\(\\varphi\\colon\\mathbb{R}^D\\to\\mathbb{R}\\) such that \\(W = \\varphi(X)\\). It holds that \\(\\mathbf{E}(Y\\,\\mathbb{I}[X\\in A]) = \\mathbf{E}(W\\,\\mathbb{I}[X\\in A]\\)), for all Borel subsets \\(A\\subseteq\\mathbb{R}^D\\). The proof that one can always find such a \\(W\\) relies on the famous Radon-Nikodym Theorem from Measure Theory, combined with the Doob-Dynkin Lemma which provides us with the function \\(\\varphi\\). As a matter of fact, one can obtain the regular conditional distribution in 3.1 as a corollary to this fact (a direct proof also relies on the Radon-Nikodym Theorem, see Billingsley (1995), Section 33, especially Theorem 33.3). Since we introduced the idea of conditioning through the viewpoint of regular conditional distributions, we can take a shortcut and define, for any integrable random variable \\(Y\\) and any random vector8 \\(X\\), \\[ \\mathbf{E}(Y\\,|\\,X = x) := \\int_0^1 Q_{Y|X}(\\tau|x)\\,\\mathrm{d}\\tau,\\qquad x\\in\\mathbb{R}^D, \\] called the conditional expectation of \\(Y\\) given \\(X=x\\), and, writing \\(\\varphi(x) := \\mathbf{E}(Y\\,|\\,X=x)\\), let \\[ \\mathbf{E}(Y\\,|\\,X) := \\varphi(X), \\] called the conditional expectation of \\(Y\\) given \\(X\\). Exercise 3.6 Show that \\(\\mathbf{E}(\\mathbb{I}[Y\\in B]\\,|\\,X=x) = \\mathbf{P}[Y\\in B\\,|\\,X=x]\\) holds for every \\(x\\in\\mathbb{R}^D\\) and all Borel sets \\(B\\subseteq \\mathbb{R}\\). \\(\\mathbf{E}\\{\\mathbf{E}(Y|X)\\,\\mathbb{I}[X\\in A]\\} = \\mathbf{E}(Y\\,\\mathbb{I}[X\\in A])\\) holds for all Borel sets \\(A\\subseteq\\mathbb{R}^D.\\) \\(\\mathbf{E}(aY + Z\\,|\\,X) = a\\mathbf{E}(Y\\,|\\,X) + \\mathbf{E}(Y\\,|\\,X)\\) for any \\(a\\in\\mathbb{R}\\). (Substitution principle) \\(\\mathbf{E}(\\psi(X,Z)\\,|\\,X=x) = \\mathbf{E}(\\psi(x,Z)\\,|\\,X=x)\\). In particular, \\(\\mathbf{E}(\\psi(X)Y\\,|\\,X=x) = \\psi(x)\\mathbf{E}(Y\\,|\\,X=x)\\) and \\(\\mathbf{E}(\\psi(X)Y\\,|\\,X) = \\psi(X)\\mathbf{E}(Y\\,|\\,X)\\). If \\(Y\\) and \\(X\\) are mutually independent, then \\(\\mathbf{E}(Y\\,|\\,X) = \\mathbf{E}(Y)\\). This holds, in particular, if \\(X\\) is constant. In fact, the definition is still valid even if \\(X\\) is an infinite sequence of random variables. This fact will be useful in the context of time series quantile regression models, where we typically condition on the entire “past” \\(\\{(Y_{s},X_{s}):s&lt;t\\}.\\)↩︎ "],["quantile-regression.html", "4 Quantile Regression", " 4 Quantile Regression Quantile Regression surfaced in its modern guise in the seminal Econometrica paper by Roger Koenker and Gilbert Bassett (Koenker and Bassett 1978). Similarly to (parametric) mean regression models, where the conditional expectation of the response given the covariates is a linear function of the latter, in its most basic form the quantile regression framework stipulates that the conditional quantile function of a scalar random variable \\(Y\\) given a \\(D\\)-dimensional random vector \\(X\\) of covariates is a linear function of the covariates: the cornerstone assumption is that, for \\(\\tau\\in \\mathscr{T}\\subseteq(0,1)\\) and \\(x\\in\\operatorname{support}(X)=:\\mathscr{X}\\),9 the representation \\[\\begin{equation} Q_{Y|X}(\\tau\\,|\\,x) = \\sum_{d=1}^D\\beta_d(\\tau) x_{d} \\equiv x&#39;\\beta(\\tau), \\tag{4.1} \\end{equation}\\] holds for some functional parameter \\(\\beta\\colon\\mathscr{T}\\to\\mathbb{R}^D\\). When \\(\\mathscr{T} = (0,1)\\), Zheng, Peng, and He (2015) call the model in equation (4.1) a globally concerned quantile regression model.10 In contrast, when \\(\\mathscr{T}\\) is a countable set (in particular when it is a singleton), they call (4.1) a locally concerned quantile regression model. Example 4.1 If \\(\\mathscr{T}=\\{^1\\!/\\!_2\\}\\), then (4.1) is a median regression model and, putting \\(\\alpha:=\\beta(^1\\!/\\!_2)\\), we can write \\[ Y = X^\\prime \\alpha + \\varepsilon \\] with \\(Q_{\\varepsilon | X}(^1\\!/\\!_2|x) = 0\\) for all \\(x\\in \\mathbb{R}^D\\). In fact, for a locally concerned quantile regression model with \\(\\mathscr{T} = \\{\\tau\\}\\), we can always write \\(Y = X^\\prime \\alpha + \\varepsilon\\) for some vector of parameters \\(\\alpha\\in\\mathbb{R}^D\\) and a random variable \\(\\varepsilon\\) satisfying \\(Q_{\\varepsilon | X}(\\tau|x) = 0\\). \\(\\blacksquare\\) The situation depicted in the above example has many useful applications. For instance, median regression allows one to obtain a robust point forecast \\(x&#39;\\widehat{\\alpha}\\) for the response when the conditional distribution of \\(Y\\) given \\(X\\) is heavy-tailed (\\(Y\\) can even fail to be integrable). Notwithstanding, a locally concerned quantile regression model fails to take full advantage of the fact that (conditional) quantile functions completely characterize the (conditional) distributions. Therefore, in what follows I’ll always have in mind the globally concerned quantile regression model (4.1) with \\(\\mathscr{T} = (0,1).\\) With this, as we have argued earlier, the joint distribution of \\(X\\) and \\(Y\\) is entirely encoded in the marginal distribution of \\(X\\) and the functional parameter \\(\\beta\\): it holds that \\[\\begin{align} \\begin{split} \\mathbf{P}[Y\\in B, X\\in A] &amp;= \\int_{A}\\int_0^1 \\mathbb{I}[x&#39;\\beta(\\tau)\\in B]\\,\\mathrm{d}\\tau\\,F_X(\\mathrm{d}x)\\\\ &amp;= \\int_0^1 \\mathbf{P}[X&#39;\\beta(\\tau)\\in B, X\\in A]\\,\\mathrm{d}\\tau \\end{split} \\tag{4.2} \\end{align}\\] for every pair of Borel sets \\(B\\subseteq \\mathbb{R}\\) and \\(A\\subseteq \\mathbb{R}^D\\). Nevertheless, in regression models one is usually uninterested in the distribution of the covariates: rather, such models aim to quantify uncertainty about \\(Y\\) given \\(X.\\) Mean regression describes this uncertainty in terms of the conditional expected value of \\(Y\\) given \\(X\\); median regression, in terms of the conditional median of \\(Y\\), and so on. The globally concerned quantile regression model, in turn, quantifies uncertainty by specifying the entire conditional distributions of \\(Y\\) given \\(X\\) (of course, via the corresponding conditional quantile functions). In this aspect, it is not too different from fully parametric generalized linear models, which also specify the conditional distribution of the response given the predictors. Quantile regression can be seen as a different take on how to achieve said specification: indeed, the model (4.1) could be described as non-parametric, since the parameter of interest is infinite dimensional, although the functional form of \\(Q_{Y|X}\\) is partially parametrized/constrained by the “linearity in \\(x\\)” assumption. At any rate, and this is not obvious at first sight, the fact is that a conditional quantile function of the form (4.1) permits a very flexible structure of dependence between \\(Y\\) and \\(X\\), allowing the covariates to modify not only the mean but also the variance, the coefficient of asymmetry, the number of modes etc of the response. To sum up, the quantile regression model is a flexible and parsimonious way to specify the structure of dependence between the response and covariates. Exercise 4.1 Prove equation (4.2). The support of \\(X\\), which we shall denote by \\(\\mathscr{X}\\), is defined by the following two conditions: 1. \\(\\mathscr{X}\\) is a closed subset of \\(\\mathbb{R}^D\\) satisfying \\(\\mathbf{P}[X\\in\\mathscr{X}]=1\\); 2. If \\(A\\subseteq\\mathbb{R}^D\\) is closed and \\(\\mathbf{P}[X\\in A] = 1\\), then \\(A\\supseteq\\mathscr{X}\\). That is \\(\\mathscr{X}\\) is the smallest closed set in \\(\\mathbb{R}^D\\) having total \\(\\mathbf{P}_X\\) probability.↩︎ Actually, they call the model globally concerned also in the case when \\(\\mathscr{T}\\) is an interval.↩︎ "],["drawbacks.html", "4.1 Drawbacks", " 4.1 Drawbacks Flexibility comes at a cost, however. When we write equation (4.1), we are implicitly restricting either the functional form of \\(\\beta\\), or the support of \\(X\\), or both, because the map \\(\\tau\\mapsto Q_{Y|X}(\\tau|x)\\) is non-decreasing and left continuous, for all \\(x\\in\\mathbb{R}^D\\). In this section I’ll discuss some of these restrictions; for conciseness, I will not repeat at every turn that \\(Y\\) is a scalar random variable and that \\(X\\) is a \\(D\\)-dimensional random vector, and that \\(Y\\) and \\(X\\) are related by \\[\\begin{equation} Q_{Y|X}(\\tau|x) = x&#39;\\beta(\\tau),\\qquad \\tau\\in(0,1),\\,x\\in\\mathbb{R}^D. \\tag{4.3} \\end{equation}\\] This is our working assumption. Example 4.2 (Quantile crossing) Suppose that \\(X = (1\\quad X_2)^\\prime\\), where \\(X_2\\) is real valued. If \\(\\beta_2(\\cdot)\\) is a non-constant function, then the support of \\(X_2\\) has a either a lower bound or an upper bound. Thus, if \\(X_2\\) is an unbounded random variable, it is necessarily the case that \\(\\beta_2(\\cdot)\\) is a constant function. Indeed, if the coefficient \\(\\beta_2\\) is not constant-in-\\(\\tau\\), then there exist two distinct quantile levels \\(\\tau_{1}&lt;\\tau_{2}\\in(0,1)\\) such that either \\(\\beta_2(\\tau_1)&lt;\\beta_2(\\tau_2)\\) or \\(\\beta_2(\\tau_1)&gt;\\beta_2(\\tau_2)\\). In any case, if \\(\\operatorname{support}(X_2)\\) were an unbounded subset of \\(\\mathbb{R}\\), then there would exist an \\(x_2\\in\\operatorname{support}(X_2)\\) such that, for \\(x = (1\\quad x_2)^\\prime\\), one would have \\(Q_Y(\\tau_1|x)&gt;Q_Y(\\tau_2|x)\\) which is forbidden since \\(\\tau\\mapsto Q_Y(\\tau|z)\\) is non-decreasing. This example easily generalizes to the scenario where \\(X\\) is of dimension \\(D&gt; 2\\): if a covariate is unbounded and can “move freely,” independently of the remaining regressors, then it must have a constant-in-\\(\\tau\\) coefficient. Notice, however, that this restriction does not necessarily apply: indeed, equation (4.3) can hold exactly (no crossing), provided there is “sufficient dependence” between the covariates, even if they are unbounded. As an example, take \\(X = (1\\quad Z\\quad Z^2)^\\prime\\) with \\(Z\\) standard normal and \\[ \\beta(\\tau) = \\begin{pmatrix} \\tfrac12Q_Z(\\tau) \\\\ 1\\\\ 2\\tau-1\\end{pmatrix} \\] for all \\(\\tau\\in(0,1)\\). Below is a scatterplot of simulated data from this model. The dashed blue lines correspond (from bottom to top) to the conditional 1st decile, median, and 9th decile. set.seed(1) n = 800 Q = function(tau,z) 1*qnorm(tau)/2 + 1*z + (2*tau-1)*(z)^(2) Z = rnorm(n) U = runif(n) Y = Q(U,Z) plot(Y~Z, pch=16, col=&quot;gray&quot;) zgrid = seq(from=min(Z), to=max(Z), length=n) for (tau in c(.1,.5,.9)){ lines(zgrid, Q(tau,zgrid), lty=&quot;dashed&quot;, col=&quot;DarkBlue&quot;, lwd=.5) } Example 4.3 (Sign restrictions) Assume that \\(X\\) is of dimension \\(D=2\\) and that \\(X_1\\) and \\(X_2\\) are non-degenerate (there’s no intercept). That is, suppose that \\(Q_Y(\\cdot|x) = \\beta_1(\\cdot)x_1 + \\beta_2(\\cdot)x_2\\). Suppose further that there are two distinct points \\(\\hat{x}, \\tilde{x}\\in \\mathscr{X}\\) with \\(\\operatorname{sign}(\\hat{x}_1)\\ne\\operatorname{sign}(\\tilde{x}_1)\\) and \\(\\hat{x}_2=\\tilde{x}_2=0\\). Then necessarily \\(\\beta_1\\) is constant-in-\\(\\tau\\): indeed, in this case the functions \\(\\tau\\mapsto \\beta_1(\\tau)\\hat{x}_1\\) and \\(\\tau\\mapsto\\beta_1(\\tau)\\tilde{x}_1\\) are both non-decreasing (being conditional quantile functions) and thus \\[ \\tau\\mapsto\\mathrm{sign}(\\hat{z}_1)\\beta_1(\\tau)\\qquad\\text{and}\\qquad\\tau\\mapsto\\mathrm{sign}(\\tilde{z}_1)\\beta_1(\\tau) \\] are both non-decrasing functions, which can only happen if \\(\\beta_1\\) is constant-in-\\(\\tau.\\) This example can be generalized to the case \\(D&gt;2\\). \\(\\blacksquare\\) In view of the preceding examples, in the quantile regression model (4.3) it is convenient to restrict attention to covariate vectors whose support is not only bounded but also a bounded subset of \\(\\mathbb{R}^D_+:= [0,+\\infty)^D\\). Notice that imposing such restrictions on the covariates may demand a restriction on the response as well, for example if \\(Y\\) is equal in distribution to some of the regressors (this is the case in stationary time series with an autoregressive component, when \\(X\\) includes lagged values of the response). These restrictions can be relaxed if we allow ourselves a less stringent approach, assuming for example that the linear specification does not hold exactly but is rather an approximation of the “true model,” valid in a relevant region of the support of \\(X\\). This can be formalized, for example, by requiring that (4.3) holds (exactly or approximately) not for all \\(x\\in\\mathscr{X}\\) but only for \\(x\\in\\mathscr{X}_0\\) where \\(\\mathscr{X}_0\\subseteq\\mathbb{R}^D\\) is a region with \\(\\mathbf{P}[X\\in\\mathscr{X}_0]&gt; 1-\\epsilon\\) where \\(\\epsilon&gt;\\) is a small constant. "],["references.html", "5 References", " 5 References Billingsley, Patrick. 1995. Probability and Measure. 3rd ed. John Wiley &amp; Sons. Koenker, Roger. 2005. Quantile Regression. Cambridge University Press. Koenker, Roger, and Gilbert Bassett. 1978. “Regression Quantiles.” Econometrica 46 (1): 33–50. Koenker, Roger, Victor Chernozhukov, X. He, and L. Peng, eds. 2017. CRC Press. van der Vaart, Aad W. 1998. Asymptotic Statistics. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press. Zheng, Qi, Limin Peng, and Xuming He. 2015. “Globally adaptive quantile regression with ultra-high dimensional data.” The Annals of Statistics 43 (5): 2225–58. https://doi.org/10.1214/15-AOS1340. "]]
